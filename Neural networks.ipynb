{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a3c429-7729-4a59-b115-f7042dd1905b",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd05b2a-f81b-478a-85bb-0986165fd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset (assuming the data is available as a CSV file named 'synthetic_data.csv')\n",
    "# Replace 'synthetic_data.csv' with the actual path to your dataset if it's different.\n",
    "try:\n",
    "    df = pd.read_csv('synthetic_data.csv')\n",
    "except FileNotFoundError:\n",
    "    # If the file is not found, generate a synthetic dataset\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=10000, n_features=50, n_informative=15, n_redundant=5, random_state=42)\n",
    "    df = pd.DataFrame(X)\n",
    "    df['target'] = y\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X.columns) # Convert back to DataFrame to keep column names\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Data loaded and prepared successfully.\")\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb3b2e-e54c-45ab-956a-f85cf24eea12",
   "metadata": {},
   "source": [
    "### Neural Network Model Definition\n",
    "\n",
    "We will define a simple sequential neural network model using TensorFlow's Keras API.\n",
    "\n",
    "The model will consist of:\n",
    "- An input layer that matches the number of features in our scaled data.\n",
    "- One or more hidden layers with a specified number of neurons and an activation function (e.g., ReLU).\n",
    "- An output layer with a single neuron and a sigmoid activation function, suitable for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9c9b5-3856-4922-bba6-081acbbe0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fa4c46-1c8e-4a7d-aae6-8f07714f5ca0",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now, we will train the neural network model using the training data (`X_train`, `y_train`) and evaluate its performance on the validation data (`X_val`, `y_val`) during the training process.\n",
    "We will use the `fit` method of the Keras model, specifying the number of epochs and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416add9-d27e-47a4-a68f-d79302be5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6739304-f7c8-4644-94bf-a993eb6fae86",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "After training, we evaluate the model's performance on the test dataset (`X_test`, `y_test`) to get an unbiased estimate of its performance on new, unseen data. We will use the following metrics:\n",
    "\n",
    "- **Accuracy**: The proportion of correctly classified instances.\n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions.\n",
    "- **Recall**: The proportion of true positive predictions among all actual positive instances.\n",
    "- **F1-score**: The harmonic mean of precision and recall, providing a balanced measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3a662-205b-4a50-985f-6a2b16526947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Predict the classes for the test data\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate other metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1-score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4a665-61cf-4445-abf0-0e9a7facc37e",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model. Hyperparameters are parameters that are not learned from the data but are set prior to the training process. Examples include the number of layers, the number of neurons in each layer, the learning rate, and the batch size.\n",
    "\n",
    "Tuning these hyperparameters can significantly impact the model's performance. We will use Keras Tuner to automate this process and find the best hyperparameters for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7ac86-c3e1-4ad4-ac57-9a98274b4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_tuner -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120aab94-9dc6-4f4f-af76-7ec12956e8eb",
   "metadata": {},
   "source": [
    "### Define the Hypermodel\n",
    "\n",
    "We will define a function that creates a Keras model with hyperparameters to be tuned. In this example, we will tune the number of neurons in the hidden layers and the learning rate of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9dcd2-efb0-48e8-a3e1-4cff260d82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_hypermodel(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    model.add(Dense(units=hp.Int('num_units_1', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    # Tune the number of units in the second Dense layer\n",
    "    model.add(Dense(units=hp.Int('num_units_2', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Instantiate the tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,  # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=2, # Number of times to train the model per trial\n",
    "    directory='my_dir', # Directory to store the results\n",
    "    project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0c168d-b766-4b0f-b427-783c0109a603",
   "metadata": {},
   "source": [
    "### Run the Hyperparameter Search\n",
    "\n",
    "We will now run the random search to find the best hyperparameters for our model. The tuner will train the model multiple times with different hyperparameter combinations and keep track of the best-performing ones based on the validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1770778-a0f2-40f3-970a-d221c873740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc1ddd-48de-412c-837e-6ca2cf5174a9",
   "metadata": {},
   "source": [
    "### Get the Best Hyperparameters and Model\n",
    "\n",
    "After the search is complete, we can retrieve the best hyperparameters found by the tuner and the corresponding best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c630a0-0857-4a04-840a-d087bd47c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The optimal number of units in the first hidden layer is {best_hps.get('num_units_1')}.\n",
    "The optimal number of units in the second hidden layer is {best_hps.get('num_units_2')}.\n",
    "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618fe02d-b8d0-4fe2-b402-20e06f7236b7",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "Understanding the importance of different features in a machine learning model can provide valuable insights into the data and the model's decision-making process. For neural networks, which are often considered black boxes, techniques like SHAP (SHapley Additive exPlanations) can help shed light on how each feature contributes to the output.\n",
    "\n",
    "SHAP values represent the average marginal contribution of each feature across all possible permutations of features. By calculating SHAP values for our model, we can identify the features that have the biggest impact on the prediction outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaae5b0-bdcc-4556-87d9-054898bda76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shap -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e86755-7f14-477c-91b7-cb12f80c4f82",
   "metadata": {},
   "source": [
    "### Calculate and Visualize SHAP Values\n",
    "\n",
    "We will use the `shap` library to calculate the SHAP values for our best model. Since our model is a Keras Sequential model, we can use `shap.DeepExplainer` or `shap.KernelExplainer`. `DeepExplainer` is faster for deep learning models, but requires a specific type of model architecture. `KernelExplainer` is more general but can be slower. We'll use `KernelExplainer` for broader applicability.\n",
    "\n",
    "We'll then visualize the SHAP values to understand which features have the most impact on the model's output. A summary plot is a good way to see the overall feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2ee43-11b8-4df7-adf4-b86ca47fa296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Select a background dataset for SHAP - a small sample of the training data is usually sufficient\n",
    "# Using X_train is fine as it is already scaled\n",
    "background = X_train.sample(100, random_state=42)\n",
    "\n",
    "# Create a SHAP explainer object\n",
    "# Use KernelExplainer for broader compatibility with Keras models\n",
    "explainer = shap.KernelExplainer(best_model.predict, background)\n",
    "\n",
    "# Calculate SHAP values for a sample of the test data\n",
    "# We'll use a small sample for demonstration purposes\n",
    "X_test_sample = X_test.sample(100, random_state=42)\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Handle potential list output from shap_values for multi-output models\n",
    "if isinstance(shap_values, list):\n",
    "  shap_values = shap_values[0]\n",
    "\n",
    "# Visualize the feature importance\n",
    "# The summary plot shows the impact of each feature on the model output\n",
    "# Pass feature names explicitly\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=X_test_sample.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480f03b-133a-480b-b14b-377eac5ba3c0",
   "metadata": {},
   "source": [
    "### Comprehensive Discussion\n",
    "\n",
    "In this notebook, we have built and evaluated a neural network model for a binary classification task. We started by loading and preparing the data, including scaling the features. We then defined a sequential neural network model with two dense hidden layers and an output layer with a sigmoid activation function.\n",
    "\n",
    "We trained the model using the Adam optimizer and binary cross-entropy loss, monitoring the accuracy on a validation set during training. After training, we evaluated the model on a separate test set and calculated key classification metrics: accuracy, precision, recall, and F1-score.\n",
    "\n",
    "To improve the model's performance, we performed hyperparameter tuning using Keras Tuner's Random Search. We tuned the number of neurons in the hidden layers and the learning rate of the optimizer. The tuner identified the best combination of these hyperparameters based on the validation accuracy.\n",
    "\n",
    "Furthermore, we explored feature importance using SHAP values to understand which features had the most significant impact on the model's predictions. This provided valuable insights into the data and the model's decision-making process.\n",
    "\n",
    "Finally, we visualized the model's performance on the test set using a confusion matrix, which clearly shows the number of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab7450-a74c-402e-8748-1e576e9ac7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions for the test set\n",
    "y_pred = (best_model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
