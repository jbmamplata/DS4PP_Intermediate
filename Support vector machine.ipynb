{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f08c92f"
   },
   "source": [
    "## Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1757760827675,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "7a7094be",
    "outputId": "27b96926-964b-4567-a75b-696412e65fa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (569, 31)\n",
      "Column names: ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', 'target']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "target                     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "df['target'] = breast_cancer.target\n",
    "\n",
    "# Display the first 5 rows\n",
    "display(df.head())\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {df.shape}\")\n",
    "\n",
    "# Print the column names\n",
    "print(f\"Column names: {df.columns.tolist()}\")\n",
    "\n",
    "# Display data types\n",
    "display(df.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "675a6f97"
   },
   "source": [
    "**Scaling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1757760843568,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "8c7dcf6f",
    "outputId": "7b3e6ce3-dc32-4439-c91c-bba3bf3686d0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.097064</td>\n",
       "      <td>-2.073335</td>\n",
       "      <td>1.269934</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>1.568466</td>\n",
       "      <td>3.283515</td>\n",
       "      <td>2.652874</td>\n",
       "      <td>2.532475</td>\n",
       "      <td>2.217515</td>\n",
       "      <td>2.255747</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.359293</td>\n",
       "      <td>2.303601</td>\n",
       "      <td>2.001237</td>\n",
       "      <td>1.307686</td>\n",
       "      <td>2.616665</td>\n",
       "      <td>2.109526</td>\n",
       "      <td>2.296076</td>\n",
       "      <td>2.750622</td>\n",
       "      <td>1.937015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.829821</td>\n",
       "      <td>-0.353632</td>\n",
       "      <td>1.685955</td>\n",
       "      <td>1.908708</td>\n",
       "      <td>-0.826962</td>\n",
       "      <td>-0.487072</td>\n",
       "      <td>-0.023846</td>\n",
       "      <td>0.548144</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>-0.868652</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.369203</td>\n",
       "      <td>1.535126</td>\n",
       "      <td>1.890489</td>\n",
       "      <td>-0.375612</td>\n",
       "      <td>-0.430444</td>\n",
       "      <td>-0.146749</td>\n",
       "      <td>1.087084</td>\n",
       "      <td>-0.243890</td>\n",
       "      <td>0.281190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.579888</td>\n",
       "      <td>0.456187</td>\n",
       "      <td>1.566503</td>\n",
       "      <td>1.558884</td>\n",
       "      <td>0.942210</td>\n",
       "      <td>1.052926</td>\n",
       "      <td>1.363478</td>\n",
       "      <td>2.037231</td>\n",
       "      <td>0.939685</td>\n",
       "      <td>-0.398008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023974</td>\n",
       "      <td>1.347475</td>\n",
       "      <td>1.456285</td>\n",
       "      <td>0.527407</td>\n",
       "      <td>1.082932</td>\n",
       "      <td>0.854974</td>\n",
       "      <td>1.955000</td>\n",
       "      <td>1.152255</td>\n",
       "      <td>0.201391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.768909</td>\n",
       "      <td>0.253732</td>\n",
       "      <td>-0.592687</td>\n",
       "      <td>-0.764464</td>\n",
       "      <td>3.283553</td>\n",
       "      <td>3.402909</td>\n",
       "      <td>1.915897</td>\n",
       "      <td>1.451707</td>\n",
       "      <td>2.867383</td>\n",
       "      <td>4.910919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133984</td>\n",
       "      <td>-0.249939</td>\n",
       "      <td>-0.550021</td>\n",
       "      <td>3.394275</td>\n",
       "      <td>3.893397</td>\n",
       "      <td>1.989588</td>\n",
       "      <td>2.175786</td>\n",
       "      <td>6.046041</td>\n",
       "      <td>4.935010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.750297</td>\n",
       "      <td>-1.151816</td>\n",
       "      <td>1.776573</td>\n",
       "      <td>1.826229</td>\n",
       "      <td>0.280372</td>\n",
       "      <td>0.539340</td>\n",
       "      <td>1.371011</td>\n",
       "      <td>1.428493</td>\n",
       "      <td>-0.009560</td>\n",
       "      <td>-0.562450</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.466770</td>\n",
       "      <td>1.338539</td>\n",
       "      <td>1.220724</td>\n",
       "      <td>0.220556</td>\n",
       "      <td>-0.313395</td>\n",
       "      <td>0.613179</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>-0.868353</td>\n",
       "      <td>-0.397100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
       "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
       "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
       "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
       "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0          3.283515        2.652874             2.532475       2.217515   \n",
       "1         -0.487072       -0.023846             0.548144       0.001392   \n",
       "2          1.052926        1.363478             2.037231       0.939685   \n",
       "3          3.402909        1.915897             1.451707       2.867383   \n",
       "4          0.539340        1.371011             1.428493      -0.009560   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                2.255747  ...      -1.359293         2.303601    2.001237   \n",
       "1               -0.868652  ...      -0.369203         1.535126    1.890489   \n",
       "2               -0.398008  ...      -0.023974         1.347475    1.456285   \n",
       "3                4.910919  ...       0.133984        -0.249939   -0.550021   \n",
       "4               -0.562450  ...      -1.466770         1.338539    1.220724   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0          1.307686           2.616665         2.109526              2.296076   \n",
       "1         -0.375612          -0.430444        -0.146749              1.087084   \n",
       "2          0.527407           1.082932         0.854974              1.955000   \n",
       "3          3.394275           3.893397         1.989588              2.175786   \n",
       "4          0.220556          -0.313395         0.613179              0.729259   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0        2.750622                 1.937015       0  \n",
       "1       -0.243890                 0.281190       0  \n",
       "2        1.152255                 0.201391       0  \n",
       "3        6.046041                 4.935010       0  \n",
       "4       -0.868353                -0.397100       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a new DataFrame with scaled features\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "df_scaled['target'] = y\n",
    "\n",
    "# Display the first 5 rows of the scaled DataFrame\n",
    "display(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f397ede6"
   },
   "source": [
    "## SVM introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1757760866733,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "129546b5",
    "outputId": "21d4e08c-e9b1-409a-84cd-a37519f4fd75"
   },
   "source": [
    "## Support Vector Machines (SVM)\n",
    "\n",
    "Support Vector Machines (SVM) are powerful supervised learning models used for both classification and regression tasks. The core idea behind SVM is to find the optimal hyperplane that separates data points into different classes (for classification) or to find a function that approximates the target variable within a certain margin of error (for regression).\n",
    "\n",
    "### SVM for Classification\n",
    "\n",
    "In SVM classification, the goal is to find a hyperplane in an N-dimensional space (where N is the number of features) that distinctly classifies the data points. The best hyperplane is the one that has the largest margin between the two classes.\n",
    "\n",
    "*   **Hyperplane:** A decision boundary that separates data points of different classes. In a 2D space, it's a line; in 3D, it's a plane; and in higher dimensions, it's a hyperplane.\n",
    "*   **Support Vectors:** These are the data points that lie closest to the hyperplane. They are the most difficult to classify and play a crucial role in defining the hyperplane and the margin.\n",
    "*   **Margin:** The region between the two hyperplanes that are parallel to the separating hyperplane and pass through the support vectors of each class.\n",
    "*   **Objective:** The objective of SVM classification is to maximize this margin. A larger margin generally leads to better generalization performance on unseen data.\n",
    "\n",
    "For cases where the data is not linearly separable, SVM uses the concept of a \"soft margin\" which allows for some misclassifications by introducing slack variables.\n",
    "\n",
    "### Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression (SVR) applies the principles of SVM to regression problems. Instead of finding a hyperplane that separates data, SVR aims to find a function that best fits the data points while allowing for a certain degree of error, specified by a parameter epsilon ($\\epsilon$).\n",
    "\n",
    "*   **Epsilon-Insensitive Tube:** SVR constructs a tube around the estimated function. The goal is to have as many data points as possible fall within this tube. The width of the tube is determined by $\\epsilon$. Errors within this tube are not penalized.\n",
    "*   **Slack Variables:** For data points that fall outside the epsilon-insensitive tube, slack variables are introduced to penalize the deviations. The objective is to minimize these deviations while also keeping the model complexity low.\n",
    "*   **Objective:** SVR seeks to find a function that has at most $\\epsilon$ deviation from the actual targets for all training data, while being as flat as possible (minimizing the weights).\n",
    "\n",
    "### The Role of Kernels\n",
    "\n",
    "Both SVM classification and SVR can handle non-linearly separable data by using kernel functions. Kernels implicitly map the data into a higher-dimensional space where it may become linearly separable. Common kernel functions include:\n",
    "\n",
    "*   **Linear Kernel:** Suitable for linearly separable data.\n",
    "*   **Polynomial Kernel:** Maps data into a higher dimension using polynomial combinations of the original features.\n",
    "*   **Radial Basis Function (RBF) Kernel:** A popular choice that can handle complex non-linear relationships.\n",
    "*   **Sigmoid Kernel:** Based on the hyperbolic tangent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9daf791f"
   },
   "source": [
    "## Classification model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "824851bd"
   },
   "source": [
    "Import the necessary class for SVM classification and instantiate a model with a linear kernel, then train it using the scaled data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1757760879672,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "65aca083",
    "outputId": "cac1c550-3c5e-4464-b3bf-6e7a7dbb42f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>SVC</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('kernel',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">kernel&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;linear&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('degree',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">degree&nbsp;</td>\n",
       "            <td class=\"value\">3</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('gamma',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">gamma&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;scale&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('coef0',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">coef0&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('shrinking',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">shrinking&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('probability',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">probability&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('cache_size',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">cache_size&nbsp;</td>\n",
       "            <td class=\"value\">200</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('decision_function_shape',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">decision_function_shape&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;ovr&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('break_ties',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">break_ties&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate an SVC object with a linear kernel\n",
    "svc_linear = SVC(kernel='linear')\n",
    "\n",
    "# Train the SVC model\n",
    "svc_linear.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1757760893506,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "f0ca7bde",
    "outputId": "adff3b72-31c7-4930-d7c4-3b3e25aa7599"
   },
   "source": [
    "## Kernel Functions in Support Vector Machines for Classification\n",
    "\n",
    "Kernel functions are a crucial component of Support Vector Machines (SVMs), particularly when dealing with non-linearly separable data. They allow SVMs to implicitly map the data into a higher-dimensional feature space without explicitly calculating the coordinates in that space. This \"kernel trick\" makes it computationally feasible to find a linear decision boundary in the higher dimension, which corresponds to a non-linear decision boundary in the original feature space.\n",
    "\n",
    "Here are some common types of kernel functions used in SVM classification:\n",
    "\n",
    "### 1. Linear Kernel\n",
    "\n",
    "The linear kernel is the simplest type of kernel. It is defined as the dot product of the input vectors:\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$\n",
    "\n",
    "This kernel is suitable for linearly separable data. Using a linear kernel is equivalent to training a standard linear SVM.\n",
    "\n",
    "### 2. Polynomial Kernel\n",
    "\n",
    "The polynomial kernel maps the input data into a higher-dimensional space using polynomial combinations of the original features. It is defined as:\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$\n",
    "\n",
    "Where:\n",
    "*   $\\gamma$ is a scaling factor (often set to 1 or $1/\\text{n\\_features}$).\n",
    "*   $r$ is a constant term (also known as the `coef0`).\n",
    "*   $d$ is the degree of the polynomial.\n",
    "\n",
    "The polynomial kernel can capture non-linear relationships in the data, and its complexity is controlled by the degree $d$.\n",
    "\n",
    "### 3. Radial Basis Function (RBF) Kernel\n",
    "\n",
    "The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is one of the most widely used kernels. It measures the similarity between two points based on their distance from a central point. The RBF kernel is defined as:\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$\n",
    "\n",
    "Where:\n",
    "*   $\\gamma$ is a parameter that controls the width of the Gaussian function. A smaller $\\gamma$ means a wider kernel, influencing more distant points. A larger $\\gamma$ means a narrower kernel, influencing only nearby points.\n",
    "\n",
    "The RBF kernel can handle complex non-linear relationships and is a good default choice when the nature of the data is unknown.\n",
    "\n",
    "### 4. Sigmoid Kernel\n",
    "\n",
    "The sigmoid kernel is based on the hyperbolic tangent function and is defined as:\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$\n",
    "\n",
    "Where:\n",
    "*   $\\gamma$ is a scaling factor.\n",
    "*   $r$ is a constant term.\n",
    "\n",
    "The sigmoid kernel is sometimes used but is less common than the RBF kernel and can sometimes behave like a linear kernel.\n",
    "\n",
    "Choosing the appropriate kernel is crucial for the performance of an SVM model and often requires experimentation and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8b483b48"
   },
   "source": [
    "\n",
    "Import the SVC class and instantiate SVC objects for each kernel type using default hyperparameters, then train each model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1757760901650,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "69f39532",
    "outputId": "e9f9091d-f874-455d-dd0a-df6b3d3d0143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC models with different kernels trained successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate SVC objects with different kernels\n",
    "svc_linear = SVC(kernel='linear')\n",
    "svc_poly = SVC(kernel='poly')\n",
    "svc_rbf = SVC(kernel='rbf')\n",
    "svc_sigmoid = SVC(kernel='sigmoid')\n",
    "\n",
    "# Train each SVC model\n",
    "svc_linear.fit(X_scaled, y)\n",
    "svc_poly.fit(X_scaled, y)\n",
    "svc_rbf.fit(X_scaled, y)\n",
    "svc_sigmoid.fit(X_scaled, y)\n",
    "\n",
    "print(\"SVC models with different kernels trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8005243"
   },
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1757760920785,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "68ae4f03",
    "outputId": "386b674f-ea0b-45f0-95cf-80ae3bcce939"
   },
   "source": [
    "## The Importance of Hyperparameter Tuning and GridSearchCV\n",
    "\n",
    "Hyperparameter tuning is a critical step in the machine learning workflow, especially for models like Support Vector Machines (SVMs). Hyperparameters are external configuration settings that are not learned from the data during the training process but significantly influence the model's performance and complexity. Examples of SVM hyperparameters include the regularization parameter `C`, the kernel type, and kernel-specific parameters like `gamma` for the RBF kernel or `degree` and `coef0` for the polynomial kernel.\n",
    "\n",
    "The performance of an SVM model is highly sensitive to the choice of these hyperparameters. Poorly chosen hyperparameters can lead to:\n",
    "\n",
    "*   **Underfitting:** If the model is too simple (e.g., low `C` value for a complex decision boundary), it may fail to capture the underlying patterns in the data, resulting in low accuracy on both training and unseen data.\n",
    "*   **Overfitting:** If the model is too complex (e.g., high `C` value or inappropriate kernel parameters), it may fit the training data too closely, including the noise, leading to excellent performance on the training data but poor generalization on new, unseen data.\n",
    "\n",
    "Therefore, finding the optimal combination of hyperparameters is essential to build a model that generalizes well to new data and achieves the best possible performance.\n",
    "\n",
    "**GridSearchCV** is a widely used technique for systematically searching for the best hyperparameters. It works by:\n",
    "\n",
    "1.  Defining a grid of hyperparameter values to explore.\n",
    "2.  Training and evaluating the model for every possible combination of hyperparameters in the grid.\n",
    "3.  Using cross-validation to assess the performance of each combination. Cross-validation helps to obtain a more reliable estimate of the model's performance on unseen data and mitigate the risk of overfitting to the training set.\n",
    "4.  Selecting the hyperparameter combination that yields the best cross-validation score.\n",
    "\n",
    "While computationally intensive, especially for large grids and datasets, GridSearchCV is a robust method for finding a good set of hyperparameters and improving the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ca45122"
   },
   "source": [
    "Import necessary libraries and define the parameter grid for the sigmoid kernel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c6f47af"
   },
   "source": [
    "Instantiate and fit GridSearchCV for each kernel and print the best parameters and scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4172,
     "status": "ok",
     "timestamp": 1757760939947,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "05c354f2",
    "outputId": "3c47421d-3fc0-43c5-f284-268d37a5d221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Linear kernel: {'C': 0.1}\n",
      "Best cross-validation score for Linear kernel: 0.9754075454122031\n",
      "\n",
      "Best parameters for Polynomial kernel: {'C': 1, 'degree': 3, 'gamma': 0.1}\n",
      "Best cross-validation score for Polynomial kernel: 0.9613724576929048\n",
      "\n",
      "Best parameters for RBF kernel: {'C': 10, 'gamma': 'scale'}\n",
      "Best cross-validation score for RBF kernel: 0.9771774569166279\n",
      "\n",
      "Best parameters for Sigmoid kernel: {'C': 10, 'coef0': 0, 'gamma': 0.01}\n",
      "Best cross-validation score for Sigmoid kernel: 0.9701288619779538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the linear kernel\n",
    "param_grid_linear = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "grid_search_linear = GridSearchCV(SVC(kernel='linear'), param_grid_linear, cv=5)\n",
    "grid_search_linear.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the polynomial kernel\n",
    "param_grid_poly = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'degree': [2, 3, 4],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "grid_search_poly = GridSearchCV(SVC(kernel='poly'), param_grid_poly, cv=5)\n",
    "grid_search_poly.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the RBF kernel\n",
    "param_grid_rbf = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "grid_search_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid_rbf, cv=5)\n",
    "grid_search_rbf.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the sigmoid kernel\n",
    "param_grid_sigmoid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'coef0': [0, 1]}\n",
    "grid_search_sigmoid = GridSearchCV(SVC(kernel='sigmoid'), param_grid_sigmoid, cv=5)\n",
    "grid_search_sigmoid.fit(X_scaled, y)\n",
    "\n",
    "# Print the best parameters and scores for each kernel\n",
    "print(\"Best parameters for Linear kernel:\", grid_search_linear.best_params_)\n",
    "print(\"Best cross-validation score for Linear kernel:\", grid_search_linear.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Polynomial kernel:\", grid_search_poly.best_params_)\n",
    "print(\"Best cross-validation score for Polynomial kernel:\", grid_search_poly.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for RBF kernel:\", grid_search_rbf.best_params_)\n",
    "print(\"Best cross-validation score for RBF kernel:\", grid_search_rbf.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Sigmoid kernel:\", grid_search_sigmoid.best_params_)\n",
    "print(\"Best cross-validation score for Sigmoid kernel:\", grid_search_sigmoid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4653707"
   },
   "source": [
    "validation and error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1757760956028,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "6ad77788",
    "outputId": "2e762bc1-47d8-4b79-e0c2-56a67370eb7c"
   },
   "source": [
    "## Model Validation and Error Metrics for Classification\n",
    "\n",
    "Evaluating the performance of a classification model is essential to understand how well it generalizes to unseen data and to compare different models. Simply training a model on the entire dataset and evaluating it on the same data can lead to an overly optimistic assessment of performance, as the model might have simply memorized the training examples (overfitting). Therefore, using appropriate validation techniques and error metrics is crucial.\n",
    "\n",
    "### Validation Techniques\n",
    "\n",
    "**Cross-Validation** is a widely used technique to assess a model's performance and robustness. It involves splitting the dataset into multiple subsets or \"folds.\" The model is trained on a subset of the folds and evaluated on the remaining fold (the validation set). This process is repeated multiple times, with each fold serving as the validation set exactly once. Common cross-validation strategies include:\n",
    "\n",
    "*   **k-Fold Cross-Validation:** The dataset is divided into $k$ equal-sized folds. The model is trained on $k-1$ folds and validated on the remaining fold. This is repeated $k$ times, and the results are averaged. This provides a more reliable estimate of the model's performance than a single train-test split.\n",
    "\n",
    "Cross-validation helps to:\n",
    "*   Obtain a more reliable estimate of the model's performance on unseen data.\n",
    "*   Reduce the variance of the performance estimate compared to a single train-test split.\n",
    "*   Make better use of the available data for both training and validation.\n",
    "\n",
    "### Error Metrics for Classification\n",
    "\n",
    "For classification tasks, several error metrics are used to evaluate a model's performance, providing different perspectives on its strengths and weaknesses. These metrics are typically calculated from the **confusion matrix**, which summarizes the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "*   **Accuracy:** The most intuitive metric, accuracy measures the proportion of correctly classified instances out of the total number of instances.\n",
    "    $\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "    Accuracy can be misleading in cases of imbalanced datasets, where one class is significantly more frequent than others.\n",
    "\n",
    "*   **Precision:** Precision measures the proportion of correctly predicted positive instances out of the total predicted positive instances. It answers the question: \"Of all the instances predicted as positive, how many were actually positive?\"\n",
    "    $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "    Precision is important when the cost of a false positive is high.\n",
    "\n",
    "*   **Recall (Sensitivity or True Positive Rate):** Recall measures the proportion of correctly predicted positive instances out of the total actual positive instances. It answers the question: \"Of all the actual positive instances, how many were correctly predicted as positive?\"\n",
    "    $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "    Recall is important when the cost of a false negative is high.\n",
    "\n",
    "*   **F1-Score:** The F1-score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall. It is a good choice when you need to consider both false positives and false negatives.\n",
    "    $\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "Choosing the appropriate error metric depends on the specific problem and the relative costs of different types of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1757760968765,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "c01b24a3",
    "outputId": "36d7a603-0508-4b56-9bb4-5bc5b3a0b31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results for Linear kernel:\n",
      "  Average Accuracy: 0.9754\n",
      "  Average Precision: 0.9700\n",
      "  Average Recall: 0.9916\n",
      "  Average F1-score: 0.9806\n",
      "\n",
      "Cross-validation results for Polynomial kernel:\n",
      "  Average Accuracy: 0.9614\n",
      "  Average Precision: 0.9496\n",
      "  Average Recall: 0.9915\n",
      "  Average F1-score: 0.9699\n",
      "\n",
      "Cross-validation results for RBF kernel:\n",
      "  Average Accuracy: 0.9772\n",
      "  Average Precision: 0.9781\n",
      "  Average Recall: 0.9860\n",
      "  Average F1-score: 0.9819\n",
      "\n",
      "Cross-validation results for Sigmoid kernel:\n",
      "  Average Accuracy: 0.9701\n",
      "  Average Precision: 0.9647\n",
      "  Average Recall: 0.9887\n",
      "  Average F1-score: 0.9765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Get the best estimators from GridSearchCV\n",
    "best_svc_linear = grid_search_linear.best_estimator_\n",
    "best_svc_poly = grid_search_poly.best_estimator_\n",
    "best_svc_rbf = grid_search_rbf.best_estimator_\n",
    "best_svc_sigmoid = grid_search_sigmoid.best_estimator_\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform cross-validation for each best estimator\n",
    "cv_results_linear = cross_validate(best_svc_linear, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_poly = cross_validate(best_svc_poly, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_rbf = cross_validate(best_svc_rbf, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_sigmoid = cross_validate(best_svc_sigmoid, X_scaled, y, cv=5, scoring=scoring)\n",
    "\n",
    "# Calculate and print the average scores\n",
    "print(\"Cross-validation results for Linear kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_linear['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_linear['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_linear['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_linear['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for Polynomial kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_poly['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_poly['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_poly['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_poly['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for RBF kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_rbf['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_rbf['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_rbf['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_rbf['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for Sigmoid kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_sigmoid['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_sigmoid['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_sigmoid['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_sigmoid['test_f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "860af9ae"
   },
   "source": [
    "## feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1757760987161,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "74519a80",
    "outputId": "e4c71f52-470f-4150-ec73-a16502824d9b"
   },
   "source": [
    "## Feature Importance in SVM Classification (Linear Kernel)\n",
    "\n",
    "For Support Vector Machines (SVMs) with a **linear kernel**, assessing feature importance is relatively straightforward. The trained linear SVM model finds a hyperplane that separates the classes, and this hyperplane is defined by a set of coefficients, one for each feature. These coefficients directly indicate the weight or importance of each feature in determining the decision boundary.\n",
    "\n",
    "*   **Magnitude of Coefficients:** The absolute magnitude of a feature's coefficient reflects its importance. A larger absolute value means that the feature has a stronger influence on the decision boundary. Features with coefficients close to zero have less impact.\n",
    "*   **Sign of Coefficients:** The sign of a coefficient indicates the direction of the relationship between the feature and the target variable. For binary classification, a positive coefficient means that increasing the feature's value makes it more likely for the instance to belong to one class, while a negative coefficient makes it more likely to belong to the other class.\n",
    "\n",
    "Therefore, by examining the coefficients of a linear SVM model, we can gain insights into which features are most important for the classification task and how they influence the model's predictions.\n",
    "\n",
    "For non-linear kernels (like RBF or polynomial), the concept of a simple linear feature importance based on coefficients is not directly applicable because the decision boundary is in a higher-dimensional space. Feature importance for non-linear SVMs is generally more complex to determine and might involve techniques like permutation importance or examining the impact of removing features. However, for a linear kernel, the coefficients provide a clear and interpretable measure of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1757760994958,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "7e145cf0",
    "outputId": "253d77c8-2c3f-4c54-e2c0-ba449f4649a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Feature Importance for Linear SVM:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "worst texture              0.457053\n",
       "worst symmetry             0.416313\n",
       "radius error               0.365099\n",
       "mean concavity             0.354242\n",
       "worst perimeter            0.353716\n",
       "worst radius               0.351924\n",
       "perimeter error            0.346983\n",
       "mean concave points        0.344767\n",
       "worst area                 0.339199\n",
       "worst concavity            0.335998\n",
       "worst smoothness           0.319742\n",
       "worst concave points       0.309327\n",
       "area error                 0.308942\n",
       "mean texture               0.298839\n",
       "compactness error          0.265730\n",
       "fractal dimension error    0.241069\n",
       "mean area                  0.240785\n",
       "mean radius                0.224539\n",
       "mean fractal dimension     0.220163\n",
       "mean perimeter             0.219614\n",
       "smoothness error           0.165472\n",
       "mean compactness           0.162494\n",
       "worst fractal dimension    0.158951\n",
       "texture error              0.123705\n",
       "symmetry error             0.102128\n",
       "concave points error       0.089969\n",
       "mean symmetry              0.046207\n",
       "concavity error            0.032140\n",
       "mean smoothness            0.026632\n",
       "worst compactness          0.003207\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Access the coefficients of the trained linear SVM model\n",
    "feature_importance_linear = pd.Series(best_svc_linear.coef_[0], index=X.columns)\n",
    "\n",
    "# Take the absolute value to represent magnitude of importance\n",
    "feature_importance_linear = feature_importance_linear.abs()\n",
    "\n",
    "# Sort the feature importance values in descending order\n",
    "sorted_feature_importance_linear = feature_importance_linear.sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted feature importance\n",
    "print(\"Sorted Feature Importance for Linear SVM:\")\n",
    "display(sorted_feature_importance_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bee73f30"
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1757761012171,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "ee5eb4ca",
    "outputId": "351f462a-a70a-4706-d543-a48c1be5824a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the DataFrame: (442, 11)\n",
      "Column names: ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6', 'target']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 442 entries, 0 to 441\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   age     442 non-null    float64\n",
      " 1   sex     442 non-null    float64\n",
      " 2   bmi     442 non-null    float64\n",
      " 3   bp      442 non-null    float64\n",
      " 4   s1      442 non-null    float64\n",
      " 5   s2      442 non-null    float64\n",
      " 6   s3      442 non-null    float64\n",
      " 7   s4      442 non-null    float64\n",
      " 8   s5      442 non-null    float64\n",
      " 9   s6      442 non-null    float64\n",
      " 10  target  442 non-null    float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 38.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age       0\n",
       "sex       0\n",
       "bmi       0\n",
       "bp        0\n",
       "s1        0\n",
       "s2        0\n",
       "s3        0\n",
       "s4        0\n",
       "s5        0\n",
       "s6        0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df_reg = pd.DataFrame(data=diabetes.data, columns=diabetes.feature_names)\n",
    "df_reg['target'] = diabetes.target\n",
    "\n",
    "# Display the first 5 rows\n",
    "display(df_reg.head())\n",
    "\n",
    "# Print the shape of the DataFrame\n",
    "print(f\"Shape of the DataFrame: {df_reg.shape}\")\n",
    "\n",
    "# Print the column names\n",
    "print(f\"Column names: {df_reg.columns.tolist()}\")\n",
    "\n",
    "# Display data types\n",
    "display(df_reg.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "display(df_reg.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1757761020450,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "7b0ebace",
    "outputId": "8913e3b0-ac87-466b-ec3c-8c7942c7cba1"
   },
   "source": [
    "## Diabetes Dataset Explanation\n",
    "\n",
    "The Diabetes dataset is a standard dataset used for regression tasks. It consists of 442 patients and 10 baseline variables, as well as a target variable representing a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "The 10 features are:\n",
    "*   `age`: age in years\n",
    "*   `sex`:\n",
    "*   `bmi`: body mass index\n",
    "*   `bp`: average blood pressure\n",
    "*   `s1`: tc, total serum cholesterol\n",
    "*   `s2`: ldl, low-density lipoproteins\n",
    "*   `s3`: hdl, high-density lipoproteins\n",
    "*   `s4`: tch, total cholesterol / HDL\n",
    "*   `s5`: ltg, possibly log of serum triglycerides level\n",
    "*   `s6`: glu, blood sugar level\n",
    "\n",
    "These features have been standardized and centered, meaning they have a mean of zero and a standard deviation of one. This is important for many machine learning models, including SVM, which can be sensitive to the scale of the input features.\n",
    "\n",
    "The **target** variable is a quantitative measure of disease progression one year after baseline, and it is what we will try to predict using Support Vector Regression (SVR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1757761028047,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "6983415a",
    "outputId": "e3ca0f7f-56a8-4496-eb36-4fca7b97cef4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800500</td>\n",
       "      <td>1.065488</td>\n",
       "      <td>1.297088</td>\n",
       "      <td>0.459841</td>\n",
       "      <td>-0.929746</td>\n",
       "      <td>-0.732065</td>\n",
       "      <td>-0.912451</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>0.418531</td>\n",
       "      <td>-0.370989</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.039567</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-1.082180</td>\n",
       "      <td>-0.553505</td>\n",
       "      <td>-0.177624</td>\n",
       "      <td>-0.402886</td>\n",
       "      <td>1.564414</td>\n",
       "      <td>-0.830301</td>\n",
       "      <td>-1.436589</td>\n",
       "      <td>-1.938479</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.793307</td>\n",
       "      <td>1.065488</td>\n",
       "      <td>0.934533</td>\n",
       "      <td>-0.119214</td>\n",
       "      <td>-0.958674</td>\n",
       "      <td>-0.718897</td>\n",
       "      <td>-0.680245</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>0.060156</td>\n",
       "      <td>-0.545154</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.872441</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-0.243771</td>\n",
       "      <td>-0.770650</td>\n",
       "      <td>0.256292</td>\n",
       "      <td>0.525397</td>\n",
       "      <td>-0.757647</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.476983</td>\n",
       "      <td>-0.196823</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113172</td>\n",
       "      <td>-0.938537</td>\n",
       "      <td>-0.764944</td>\n",
       "      <td>0.459841</td>\n",
       "      <td>0.082726</td>\n",
       "      <td>0.327890</td>\n",
       "      <td>0.171178</td>\n",
       "      <td>-0.054499</td>\n",
       "      <td>-0.672502</td>\n",
       "      <td>-0.980568</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.800500  1.065488  1.297088  0.459841 -0.929746 -0.732065 -0.912451   \n",
       "1 -0.039567 -0.938537 -1.082180 -0.553505 -0.177624 -0.402886  1.564414   \n",
       "2  1.793307  1.065488  0.934533 -0.119214 -0.958674 -0.718897 -0.680245   \n",
       "3 -1.872441 -0.938537 -0.243771 -0.770650  0.256292  0.525397 -0.757647   \n",
       "4  0.113172 -0.938537 -0.764944  0.459841  0.082726  0.327890  0.171178   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.054499  0.418531 -0.370989   151.0  \n",
       "1 -0.830301 -1.436589 -1.938479    75.0  \n",
       "2 -0.054499  0.060156 -0.545154   141.0  \n",
       "3  0.721302  0.476983 -0.196823   206.0  \n",
       "4 -0.054499 -0.672502 -0.980568   135.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate the features (X) and the target variable (y)\n",
    "X_reg = df_reg.drop('target', axis=1)\n",
    "y_reg = df_reg['target']\n",
    "\n",
    "# Initialize a StandardScaler\n",
    "scaler_reg = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_reg_scaled = scaler_reg.fit_transform(X_reg)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "df_reg_scaled = pd.DataFrame(X_reg_scaled, columns=X_reg.columns)\n",
    "df_reg_scaled['target'] = y_reg\n",
    "\n",
    "# Display the first 5 rows of the scaled DataFrame\n",
    "display(df_reg_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b334524"
   },
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1757761041864,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "d89bd20e",
    "outputId": "28ef2878-0968-4518-a34f-aaa1c7a61f7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial SVR model with linear kernel trained successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Instantiate an SVR object with a linear kernel\n",
    "svr_linear = SVR(kernel='linear')\n",
    "\n",
    "# Train the SVR model\n",
    "svr_linear.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "print(\"Initial SVR model with linear kernel trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebafd8bb"
   },
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1757761060435,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "d068d69d",
    "outputId": "502d2cbd-6898-4af1-accf-cc4a7666ff47"
   },
   "source": [
    "## Kernel Functions in Support Vector Regression (SVR)\n",
    "\n",
    "Similar to Support Vector Machines (SVM) for classification, **Kernel functions** play a vital role in Support Vector Regression (SVR). They enable SVR to model non-linear relationships between features and the target variable without explicitly transforming the data into a higher-dimensional space. This is achieved through the \"kernel trick,\" which computes the dot product of the data points in a high-dimensional feature space implicitly.\n",
    "\n",
    "In SVR, the goal is to find a function that approximates the target values while minimizing the error within a defined margin ($\\epsilon$). Kernels allow SVR to find complex, non-linear functions that fit the data well, even when the relationship is not linear in the original feature space.\n",
    "\n",
    "Here are the common kernel types used in SVR, with characteristics relevant to regression:\n",
    "\n",
    "### 1. Linear Kernel\n",
    "\n",
    "The linear kernel calculates a linear relationship between the input features and the target variable. It is suitable when the relationship between the features and the target can be approximated by a straight line or a hyperplane in the original feature space.\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$\n",
    "\n",
    "### 2. Polynomial Kernel\n",
    "\n",
    "The polynomial kernel allows SVR to fit non-linear relationships by considering polynomial combinations of the features. The degree of the polynomial ($d$) determines the complexity of the non-linear mapping.\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$\n",
    "\n",
    "This kernel can capture curved or more complex patterns in the data.\n",
    "\n",
    "### 3. Radial Basis Function (RBF) Kernel\n",
    "\n",
    "The RBF kernel is a powerful and versatile kernel that can model highly non-linear relationships. It considers the similarity between data points based on their distance. The `gamma` parameter controls the influence of individual training samples; a smaller `gamma` means a larger influence and a smoother decision boundary (or regression function in this case), while a larger `gamma` means a smaller influence and a more complex function.\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$\n",
    "\n",
    "The RBF kernel is often a good default choice for SVR when the nature of the non-linearity is unknown.\n",
    "\n",
    "### 4. Sigmoid Kernel\n",
    "\n",
    "The sigmoid kernel, based on the hyperbolic tangent function, can also be used in SVR to model non-linear relationships. However, it's less commonly used than the RBF kernel in practice and its behavior can sometimes be similar to a linear kernel, especially for certain parameter values.\n",
    "\n",
    "$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$\n",
    "\n",
    "Choosing the appropriate kernel and tuning its hyperparameters are essential steps in building an effective SVR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1757761068888,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "785b5285",
    "outputId": "d55137f9-f9d0-49e6-fea2-29c1dade2739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR models with different kernels trained successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Instantiate SVR objects with different kernels using default hyperparameters\n",
    "svr_linear = SVR(kernel='linear')\n",
    "svr_poly = SVR(kernel='poly')\n",
    "svr_rbf = SVR(kernel='rbf')\n",
    "svr_sigmoid = SVR(kernel='sigmoid')\n",
    "\n",
    "# Train each SVR model using the scaled regression data\n",
    "svr_linear.fit(X_reg_scaled, y_reg)\n",
    "svr_poly.fit(X_reg_scaled, y_reg)\n",
    "svr_rbf.fit(X_reg_scaled, y_reg)\n",
    "svr_sigmoid.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "print(\"SVR models with different kernels trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fb154be"
   },
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1757761087566,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "45fb12d1",
    "outputId": "1e4ec9e5-747a-439c-e7dc-21485431fab8"
   },
   "source": [
    "## Hyperparameter Tuning for Support Vector Regression (SVR)\n",
    "\n",
    "Hyperparameter tuning is as crucial for Support Vector Regression (SVR) as it is for SVM classification. SVR models have several hyperparameters that significantly influence their performance, including the model's complexity, its ability to fit the training data, and its generalization to unseen data.\n",
    "\n",
    "Key hyperparameters for SVR include:\n",
    "\n",
    "*   **C (Regularization Parameter):** This parameter controls the trade-off between fitting the training data and maintaining a smooth regression function. A smaller `C` creates a wider margin and a smoother function but may lead to underfitting. A larger `C` allows for fitting the training data more precisely (potentially including noise) but can lead to overfitting.\n",
    "*   **epsilon ($\\epsilon$):** This parameter defines the epsilon-insensitive tube. Errors within this tube are not penalized. A larger $\\epsilon$ leads to a simpler model with fewer support vectors but might ignore important variations in the data. A smaller $\\epsilon$ forces the model to fit the training data more closely, potentially leading to a more complex model and sensitivity to noise.\n",
    "*   **Kernel-Specific Parameters:**\n",
    "    *   **gamma:** For RBF, polynomial, and sigmoid kernels, `gamma` influences the shape of the decision boundary (or regression function). It affects how far the influence of a single training example reaches.\n",
    "    *   **degree:** For the polynomial kernel, `degree` is the degree of the polynomial function. Higher degrees allow for more complex non-linear relationships but increase the risk of overfitting.\n",
    "    *   **coef0:** For polynomial and sigmoid kernels, `coef0` is a constant term that affects the shape of the kernel function.\n",
    "\n",
    "Tuning these hyperparameters is essential to find the optimal balance between model complexity and fitting the data, ultimately leading to better generalization performance on new, unseen data. Techniques like GridSearchCV systematically explore different hyperparameter combinations to identify the best-performing set based on a chosen evaluation metric (e.g., Mean Squared Error for regression).\n",
    "\n",
    "### Parameter Grids for SVR Kernels\n",
    "\n",
    "To perform hyperparameter tuning using GridSearchCV, we need to define a grid of potential values for the relevant hyperparameters for each kernel.\n",
    "\n",
    "*   **Linear Kernel:** The primary hyperparameter for the linear kernel is `C`.\n",
    "*   **Polynomial Kernel:** Relevant hyperparameters include `C`, `degree`, and `coef0`.\n",
    "*   **RBF Kernel:** Relevant hyperparameters include `C` and `gamma`.\n",
    "*   **Sigmoid Kernel:** Relevant hyperparameters include `C`, `gamma`, and `coef0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29763,
     "status": "ok",
     "timestamp": 1757761127290,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "fff048d2",
    "outputId": "aa5334ef-8887-4fd4-e730-4b7fb8d033a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Linear SVR: {'C': 1}\n",
      "Best cross-validation score (Negative MSE) for Linear SVR: -3026.59246296212\n",
      "\n",
      "Best parameters for Polynomial SVR: {'C': 0.1, 'coef0': 10, 'degree': 3}\n",
      "Best cross-validation score (Negative MSE) for Polynomial SVR: -2990.0804032835194\n",
      "\n",
      "Best parameters for RBF SVR: {'C': 100, 'gamma': 0.01}\n",
      "Best cross-validation score (Negative MSE) for RBF SVR: -2937.417624601308\n",
      "\n",
      "Best parameters for Sigmoid SVR: {'C': 10, 'coef0': 0, 'gamma': 0.1}\n",
      "Best cross-validation score (Negative MSE) for Sigmoid SVR: -3074.81708677377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define parameter grids for each SVR kernel\n",
    "param_grid_linear_reg = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_poly_reg = {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'coef0': [0, 1, 10]}\n",
    "param_grid_rbf_reg = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "param_grid_sigmoid_reg = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'coef0': [0, 1]}\n",
    "\n",
    "# Instantiate and fit GridSearchCV for each kernel\n",
    "grid_search_linear_reg = GridSearchCV(SVR(kernel='linear'), param_grid_linear_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_linear_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_poly_reg = GridSearchCV(SVR(kernel='poly'), param_grid_poly_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_poly_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_rbf_reg = GridSearchCV(SVR(kernel='rbf'), param_grid_rbf_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rbf_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_sigmoid_reg = GridSearchCV(SVR(kernel='sigmoid'), param_grid_sigmoid_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_sigmoid_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "# Print the best hyperparameters and best cross-validation score for each kernel\n",
    "print(\"Best parameters for Linear SVR:\", grid_search_linear_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Linear SVR:\", grid_search_linear_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Polynomial SVR:\", grid_search_poly_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Polynomial SVR:\", grid_search_poly_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for RBF SVR:\", grid_search_rbf_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for RBF SVR:\", grid_search_rbf_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Sigmoid SVR:\", grid_search_sigmoid_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Sigmoid SVR:\", grid_search_sigmoid_reg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fea2e58"
   },
   "source": [
    "## Validation and error metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757761148614,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "1c079cff",
    "outputId": "cb156cc0-57a9-475b-bbeb-bc39ae42b5c1"
   },
   "source": [
    "## Model Validation and Error Metrics for Regression\n",
    "\n",
    "In regression tasks, the goal is to predict a continuous target variable. Evaluating the performance of a regression model requires different metrics compared to classification. Similar to classification, proper validation techniques are essential to ensure that the model's performance estimate is reliable and that it generalizes well to unseen data.\n",
    "\n",
    "### Validation Techniques\n",
    "\n",
    "As discussed for classification, **Cross-Validation** is a fundamental technique for assessing the performance and robustness of a regression model. By splitting the data into multiple folds and training/validating on different combinations, we obtain a more reliable estimate of how the model will perform on new data, reducing the risk of overfitting. k-Fold cross-validation is commonly used, where the dataset is divided into $k$ folds, and the model is trained on $k-1$ folds and evaluated on the remaining fold, repeated $k$ times.\n",
    "\n",
    "### Error Metrics for Regression\n",
    "\n",
    "Several metrics are used to quantify the difference between the predicted values ($\\hat{y}$) and the actual target values ($y$).\n",
    "\n",
    "*   **Mean Squared Error (MSE):** MSE is one of the most common regression error metrics. It measures the average of the squared differences between the predicted and actual values.\n",
    "    $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "    MSE penalizes larger errors more heavily due to the squaring. A lower MSE indicates a better model fit. The unit of MSE is the square of the unit of the target variable, which can make it less intuitive to interpret directly.\n",
    "\n",
    "*   **R-squared ($R^2$) Score:** The R-squared score, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variables. It provides an indication of how well the model fits the observed data.\n",
    "    $R^2 = 1 - \\frac{\\text{Sum of Squares of Residuals}}{\\text{Total Sum of Squares}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "    Where $\\bar{y}$ is the mean of the actual target values.\n",
    "    The R-squared score ranges from 0 to 1 (although it can be negative in some cases, indicating a very poor fit). An R-squared of 1 means the model perfectly predicts the target variable's variance. An R-squared of 0 means the model does not explain any of the variance in the target variable (it performs no better than simply predicting the mean). A higher R-squared generally indicates a better model fit.\n",
    "\n",
    "When evaluating regression models, it's often beneficial to consider both MSE (or its square root, RMSE, which is in the same unit as the target) and R-squared to get a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1757761160150,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "a604698f",
    "outputId": "aec43cf4-851f-49aa-f83e-6ed1db8fdb22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-validation Results for Tuned SVR Models:\n",
      "Linear SVR: MSE = 3026.5925, R-squared = 0.4772\n",
      "Polynomial SVR: MSE = 2990.0804, R-squared = 0.4831\n",
      "RBF SVR: MSE = 2937.4176, R-squared = 0.4923\n",
      "Sigmoid SVR: MSE = 3074.8171, R-squared = 0.4697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Retrieve the best estimators from the completed GridSearchCV objects\n",
    "best_svr_linear = grid_search_linear_reg.best_estimator_\n",
    "best_svr_poly = grid_search_poly_reg.best_estimator_\n",
    "best_svr_rbf = grid_search_rbf_reg.best_estimator_\n",
    "best_svr_sigmoid = grid_search_sigmoid_reg.best_estimator_\n",
    "\n",
    "# Define the scoring metrics for regression\n",
    "scoring_reg = ['neg_mean_squared_error', 'r2']\n",
    "\n",
    "# Perform 5-fold cross-validation for each best SVR estimator\n",
    "cv_results_linear_reg = cross_validate(best_svr_linear, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_poly_reg = cross_validate(best_svr_poly, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_rbf_reg = cross_validate(best_svr_rbf, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_sigmoid_reg = cross_validate(best_svr_sigmoid, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "\n",
    "# Calculate the average MSE and R-squared for each kernel\n",
    "average_mse_linear_reg = -cv_results_linear_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_linear_reg = cv_results_linear_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_poly_reg = -cv_results_poly_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_poly_reg = cv_results_poly_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_rbf_reg = -cv_results_rbf_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_rbf_reg = cv_results_rbf_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_sigmoid_reg = -cv_results_sigmoid_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_sigmoid_reg = cv_results_sigmoid_reg['test_r2'].mean()\n",
    "\n",
    "# Print the average MSE and R-squared for each SVR kernel\n",
    "print(\"Average Cross-validation Results for Tuned SVR Models:\")\n",
    "print(f\"Linear SVR: MSE = {average_mse_linear_reg:.4f}, R-squared = {average_r2_linear_reg:.4f}\")\n",
    "print(f\"Polynomial SVR: MSE = {average_mse_poly_reg:.4f}, R-squared = {average_r2_poly_reg:.4f}\")\n",
    "print(f\"RBF SVR: MSE = {average_mse_rbf_reg:.4f}, R-squared = {average_r2_rbf_reg:.4f}\")\n",
    "print(f\"Sigmoid SVR: MSE = {average_mse_sigmoid_reg:.4f}, R-squared = {average_r2_sigmoid_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93ad7294"
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1757761175451,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "3348f346",
    "outputId": "29bb5d7d-f8c9-4f73-d4fe-3eee116fe369"
   },
   "source": [
    "## Feature Importance in Support Vector Regression (SVR)\n",
    "\n",
    "Assessing feature importance in Support Vector Regression (SVR) provides insights into which input features have the most significant impact on the predicted continuous target variable. The approach to determining feature importance depends heavily on the chosen kernel.\n",
    "\n",
    "For SVR models using a **linear kernel**, feature importance can be directly interpreted from the model's **coefficients**. Similar to linear regression or linear SVM classification, a linear SVR model learns a linear function of the input features to predict the target.\n",
    "\n",
    "The learned linear function can be represented as:\n",
    "\n",
    "$f(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "\n",
    "Where:\n",
    "*   $\\mathbf{x}$ is the input feature vector.\n",
    "*   $\\mathbf{w}$ is the vector of coefficients (weights).\n",
    "*   $b$ is the intercept.\n",
    "\n",
    "The elements of the coefficient vector $\\mathbf{w}$ correspond to the weights assigned to each feature.\n",
    "\n",
    "*   **Magnitude of Coefficients:** The absolute value of a feature's coefficient ($|w_i|$) indicates the strength of its influence on the predicted target value. A larger absolute coefficient means that a unit change in that feature has a larger effect on the prediction. Features with coefficients close to zero have minimal impact.\n",
    "*   **Sign of Coefficients:** The sign of a coefficient ($w_i$) indicates the direction of the relationship between the feature and the target. A positive coefficient means that increasing the feature's value leads to an increase in the predicted target value (assuming other features are held constant), while a negative coefficient means that increasing the feature's value leads to a decrease in the predicted target value.\n",
    "\n",
    "Therefore, for linear SVR, the absolute values of the coefficients provide a straightforward measure of feature importance, allowing us to rank features based on their influence on the regression output.\n",
    "\n",
    "For SVR models with non-linear kernels (such as polynomial or RBF), the concept of a simple linear coefficient-based feature importance is not directly applicable because the model operates in a transformed, higher-dimensional space. Determining feature importance in these cases typically requires more complex methods, such as permutation importance, or analyzing the sensitivity of the model's output to changes in individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1757761185648,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "4d4e2407",
    "outputId": "d0a35756-a98d-4fe9-8f8c-8a6d65cd1d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Feature Importance for Linear SVR:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s5     21.541068\n",
       "bmi    20.861853\n",
       "bp     15.922391\n",
       "sex    11.955362\n",
       "s3      8.627998\n",
       "s4      6.509319\n",
       "s2      4.805351\n",
       "s6      4.200074\n",
       "s1      3.949230\n",
       "age     0.104373\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Access the coefficients of the trained best_svr_linear model\n",
    "# The coefficients are stored in the 'coef_' attribute. Since it's a single output, we take the first row [0].\n",
    "feature_importance_linear_reg = pd.Series(best_svr_linear.coef_[0], index=X_reg.columns)\n",
    "\n",
    "# Calculate the absolute value to represent the magnitude of feature importance\n",
    "feature_importance_linear_reg = feature_importance_linear_reg.abs()\n",
    "\n",
    "# Sort the feature importance values in descending order\n",
    "sorted_feature_importance_linear_reg = feature_importance_linear_reg.sort_values(ascending=False)\n",
    "\n",
    "# Print and display the sorted feature importance\n",
    "print(\"Sorted Feature Importance for Linear SVR:\")\n",
    "display(sorted_feature_importance_linear_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eac0f6ff"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a markdown cell to present the theoretical introduction to SVMs for classification, including core concepts and kernels, reusing the content from previous subtasks, as per instruction 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1757761212944,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "ba1a380d",
    "outputId": "8de8c824-e7a6-44a3-a622-7f20b4abeeba"
   },
   "source": [
    "## Support Vector Machines (SVM) for Classification\n",
    "\n",
    "In SVM classification, the goal is to find the optimal hyperplane that separates data points belonging to different classes with the largest possible margin.\n",
    "\n",
    "*   **Hyperplane:** A decision boundary in an N-dimensional space that separates data points.\n",
    "*   **Support Vectors:** Data points closest to the hyperplane that influence its position and the margin.\n",
    "*   **Margin:** The region between parallel hyperplanes that pass through the support vectors. Maximizing the margin is the core objective.\n",
    "\n",
    "For non-linearly separable data, SVM utilizes the \"kernel trick\" to implicitly map data into a higher-dimensional space where a linear separation might be possible.\n",
    "\n",
    "### Classification Kernel Functions\n",
    "\n",
    "Kernel functions allow SVMs to model non-linear decision boundaries. Common kernels include:\n",
    "\n",
    "*   **Linear Kernel:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$. Suitable for linearly separable data.\n",
    "*   **Polynomial Kernel:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$. Maps data using polynomial combinations, controlled by degree $d$.\n",
    "*   **Radial Basis Function (RBF) Kernel:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$. A versatile kernel for complex non-linear relationships, controlled by $\\gamma$.\n",
    "*   **Sigmoid Kernel:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$. Based on the hyperbolic tangent function.\n",
    "\n",
    "Choosing the right kernel and tuning its parameters is crucial for classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4452,
     "status": "ok",
     "timestamp": 1757761228396,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "8bd62de2",
    "outputId": "ac6a3bdb-d0c0-49c9-a11c-00ca3b6d6b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Linear kernel: {'C': 0.1}\n",
      "Best cross-validation score for Linear kernel: 0.9754075454122031\n",
      "\n",
      "Best parameters for Polynomial kernel: {'C': 1, 'coef0': 1, 'degree': 3}\n",
      "Best cross-validation score for Polynomial kernel: 0.9807017543859649\n",
      "\n",
      "Best parameters for RBF kernel: {'C': 10, 'gamma': 0.01}\n",
      "Best cross-validation score for RBF kernel: 0.9789318428815401\n",
      "\n",
      "Best parameters for Sigmoid kernel: {'C': 10, 'coef0': 0, 'gamma': 0.01}\n",
      "Best cross-validation score for Sigmoid kernel: 0.9701288619779538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define parameter grids for each classification kernel\n",
    "param_grid_linear = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_poly = {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'coef0': [0, 1, 10]}\n",
    "param_grid_rbf = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "param_grid_sigmoid = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'coef0': [0, 1]}\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the linear kernel\n",
    "grid_search_linear = GridSearchCV(SVC(kernel='linear'), param_grid_linear, cv=5)\n",
    "grid_search_linear.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the polynomial kernel\n",
    "grid_search_poly = GridSearchCV(SVC(kernel='poly'), param_grid_poly, cv=5)\n",
    "grid_search_poly.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the RBF kernel\n",
    "grid_search_rbf = GridSearchCV(SVC(kernel='rbf'), param_grid_rbf, cv=5)\n",
    "grid_search_rbf.fit(X_scaled, y)\n",
    "\n",
    "# Instantiate and fit GridSearchCV for the sigmoid kernel\n",
    "grid_search_sigmoid = GridSearchCV(SVC(kernel='sigmoid'), param_grid_sigmoid, cv=5)\n",
    "grid_search_sigmoid.fit(X_scaled, y)\n",
    "\n",
    "# Print the best parameters and scores for each kernel\n",
    "print(\"Best parameters for Linear kernel:\", grid_search_linear.best_params_)\n",
    "print(\"Best cross-validation score for Linear kernel:\", grid_search_linear.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Polynomial kernel:\", grid_search_poly.best_params_)\n",
    "print(\"Best cross-validation score for Polynomial kernel:\", grid_search_poly.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for RBF kernel:\", grid_search_rbf.best_params_)\n",
    "print(\"Best cross-validation score for RBF kernel:\", grid_search_rbf.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Sigmoid kernel:\", grid_search_sigmoid.best_params_)\n",
    "print(\"Best cross-validation score for Sigmoid kernel:\", grid_search_sigmoid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d823308"
   },
   "source": [
    "**Reasoning**:\n",
    "Explain classification validation techniques and error metrics, reusing the content from the previous subtask, as per instruction 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1757761237742,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "966b3e13",
    "outputId": "03b44677-5309-46aa-f247-c748899b33d3"
   },
   "source": [
    "## Classification Model Validation and Error Metrics\n",
    "\n",
    "To reliably evaluate the performance of our classification models and understand how well they generalize to unseen data, we use **Cross-Validation**. Specifically, we employ k-Fold Cross-Validation, where the dataset is split into $k$ folds, and the model is trained on $k-1$ folds and validated on the remaining fold, rotating through all folds. This provides a more robust performance estimate than a single train-test split.\n",
    "\n",
    "We will use the following error metrics, calculated from the confusion matrix, to evaluate the tuned classification models:\n",
    "\n",
    "*   **Accuracy:** The proportion of correctly classified instances.\n",
    "    $\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "*   **Precision:** The proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "    $\\text{Precision} = \\frac{TP}{TP + FP}$\n",
    "*   **Recall (Sensitivity):** The proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "    $\\text{Recall} = \\frac{TP}{TP + FN}$\n",
    "*   **F1-Score:** The harmonic mean of Precision and Recall, balancing both metrics.\n",
    "    $\\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1757761248881,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "8dbb03b9",
    "outputId": "e2e1c29c-bbf3-437b-d0a6-d38579d92da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results for Linear kernel:\n",
      "  Average Accuracy: 0.9754\n",
      "  Average Precision: 0.9700\n",
      "  Average Recall: 0.9916\n",
      "  Average F1-score: 0.9806\n",
      "\n",
      "Cross-validation results for Polynomial kernel:\n",
      "  Average Accuracy: 0.9614\n",
      "  Average Precision: 0.9496\n",
      "  Average Recall: 0.9915\n",
      "  Average F1-score: 0.9699\n",
      "\n",
      "Cross-validation results for RBF kernel:\n",
      "  Average Accuracy: 0.9772\n",
      "  Average Precision: 0.9781\n",
      "  Average Recall: 0.9860\n",
      "  Average F1-score: 0.9819\n",
      "\n",
      "Cross-validation results for Sigmoid kernel:\n",
      "  Average Accuracy: 0.9701\n",
      "  Average Precision: 0.9647\n",
      "  Average Recall: 0.9887\n",
      "  Average F1-score: 0.9765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Get the best estimators from GridSearchCV\n",
    "best_svc_linear = grid_search_linear.best_estimator_\n",
    "best_svc_poly = grid_search_poly.best_estimator_\n",
    "best_svc_rbf = grid_search_rbf.best_estimator_\n",
    "best_svc_sigmoid = grid_search_sigmoid.best_estimator_\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Perform cross-validation for each best estimator\n",
    "cv_results_linear = cross_validate(best_svc_linear, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_poly = cross_validate(best_svc_poly, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_rbf = cross_validate(best_svc_rbf, X_scaled, y, cv=5, scoring=scoring)\n",
    "cv_results_sigmoid = cross_validate(best_svc_sigmoid, X_scaled, y, cv=5, scoring=scoring)\n",
    "\n",
    "# Calculate and print the average scores\n",
    "print(\"Cross-validation results for Linear kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_linear['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_linear['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_linear['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_linear['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for Polynomial kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_poly['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_poly['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_poly['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_poly['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for RBF kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_rbf['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_rbf['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_rbf['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_rbf['test_f1'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation results for Sigmoid kernel:\")\n",
    "print(f\"  Average Accuracy: {cv_results_sigmoid['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Average Precision: {cv_results_sigmoid['test_precision'].mean():.4f}\")\n",
    "print(f\"  Average Recall: {cv_results_sigmoid['test_recall'].mean():.4f}\")\n",
    "print(f\"  Average F1-score: {cv_results_sigmoid['test_f1'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd0d7e63"
   },
   "source": [
    "**Reasoning**:\n",
    "Discuss feature importance in linear SVM classification, reusing the content from the previous subtask, as per instruction 6.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1757761258323,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "417808ae",
    "outputId": "0e2ea969-c6c3-4fbc-8c42-6426858b6559"
   },
   "source": [
    "## Feature Importance in Linear SVM Classification\n",
    "\n",
    "For Support Vector Machines with a **linear kernel**, we can assess feature importance by examining the **coefficients** assigned to each feature by the trained model. These coefficients represent the weight or influence of each feature on the decision boundary that separates the classes.\n",
    "\n",
    "*   The **absolute magnitude** of a coefficient indicates the strength of a feature's importance. Larger absolute values correspond to features that have a greater impact on the classification decision.\n",
    "*   The **sign** of a coefficient indicates the direction of the relationship between the feature and the target class.\n",
    "\n",
    "By analyzing these coefficients, we can identify which features are most influential in the linear SVM classification model. This interpretation is straightforward for linear kernels but not directly applicable to non-linear kernels (like RBF or polynomial) where feature interactions are more complex due to the mapping into a higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757761269153,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "230b444a",
    "outputId": "d92be6e2-1f63-4cc1-d7d3-0d284fc5c0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Feature Importance for Linear SVM:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>0.457053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>0.416313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>0.365099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>0.354242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>0.353716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>0.351924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>0.346983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>0.344767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>0.339199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>0.335998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>0.319742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>0.309327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>0.308942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>0.298839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>0.265730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>0.241069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>0.240785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>0.224539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>0.220163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>0.219614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>0.165472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>0.162494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>0.158951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>0.123705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>0.102128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>0.089969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>0.046207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>0.032140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>0.026632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>0.003207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> float64</label>"
      ],
      "text/plain": [
       "worst texture              0.457053\n",
       "worst symmetry             0.416313\n",
       "radius error               0.365099\n",
       "mean concavity             0.354242\n",
       "worst perimeter            0.353716\n",
       "worst radius               0.351924\n",
       "perimeter error            0.346983\n",
       "mean concave points        0.344767\n",
       "worst area                 0.339199\n",
       "worst concavity            0.335998\n",
       "worst smoothness           0.319742\n",
       "worst concave points       0.309327\n",
       "area error                 0.308942\n",
       "mean texture               0.298839\n",
       "compactness error          0.265730\n",
       "fractal dimension error    0.241069\n",
       "mean area                  0.240785\n",
       "mean radius                0.224539\n",
       "mean fractal dimension     0.220163\n",
       "mean perimeter             0.219614\n",
       "smoothness error           0.165472\n",
       "mean compactness           0.162494\n",
       "worst fractal dimension    0.158951\n",
       "texture error              0.123705\n",
       "symmetry error             0.102128\n",
       "concave points error       0.089969\n",
       "mean symmetry              0.046207\n",
       "concavity error            0.032140\n",
       "mean smoothness            0.026632\n",
       "worst compactness          0.003207\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Access the coefficients of the trained linear SVM model\n",
    "feature_importance_linear = pd.Series(best_svc_linear.coef_[0], index=X.columns)\n",
    "\n",
    "# Take the absolute value to represent magnitude of importance\n",
    "feature_importance_linear = feature_importance_linear.abs()\n",
    "\n",
    "# Sort the feature importance values in descending order\n",
    "sorted_feature_importance_linear = feature_importance_linear.sort_values(ascending=False)\n",
    "\n",
    "# Display the sorted feature importance\n",
    "print(\"Sorted Feature Importance for Linear SVM:\")\n",
    "display(sorted_feature_importance_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1757761281239,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "31c3d059",
    "outputId": "cfdc8151-8dee-4bf9-cb39-a86361c0e2aa"
   },
   "source": [
    "## Support Vector Regression (SVR) Example: Diabetes Dataset\n",
    "\n",
    "Now, we will explore Support Vector Machines applied to a regression problem using the **Diabetes Dataset**. This dataset is a standard benchmark for regression and contains physiological measurements and a quantitative measure of disease progression for 442 diabetes patients.\n",
    "\n",
    "The dataset includes 10 baseline features that have been standardized and centered, and the target variable is a continuous measure of disease progression one year after baseline. This task will demonstrate how SVR can be used to predict a continuous outcome based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1757761301230,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "5cfdec44",
    "outputId": "d94364dc-ebed-4d5b-9b7e-13b6c035f3f0"
   },
   "source": [
    "## Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression (SVR) is the adaptation of SVM principles to regression problems. Instead of finding a hyperplane to separate classes, SVR aims to find a function that approximates the target variable while allowing for a certain tolerance for errors, defined by $\\epsilon$.\n",
    "\n",
    "*   **Epsilon-Insensitive Tube:** SVR constructs a tube around the predicted function. Errors within this tube (with width $2\\epsilon$) are not penalized. The goal is to minimize the errors outside this tube and find a function that is as flat as possible.\n",
    "*   **Support Vectors:** Similar to classification, support vectors are the data points that lie on or outside the epsilon-insensitive tube. They are the critical points that define the regression function.\n",
    "\n",
    "SVR also utilizes kernel functions to handle non-linear relationships between features and the continuous target variable.\n",
    "\n",
    "### Regression Kernel Functions\n",
    "\n",
    "The same kernel functions used in SVM classification can be applied to SVR to model non-linear relationships in the data:\n",
    "\n",
    "*   **Linear Kernel:** Suitable for linear relationships between features and the target.\n",
    "    $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$\n",
    "*   **Polynomial Kernel:** Used for modeling polynomial relationships.\n",
    "    $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$\n",
    "*   **Radial Basis Function (RBF) Kernel:** A flexible kernel for complex non-linear relationships, controlled by $\\gamma$.\n",
    "    $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$\n",
    "*   **Sigmoid Kernel:** Can model non-linear relationships, though less common than RBF.\n",
    "    $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)$\n",
    "\n",
    "Selecting the appropriate kernel and tuning its parameters (`C`, $\\epsilon$, and kernel-specific parameters like `gamma`, `degree`, `coef0`) are crucial for optimizing SVR performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26579,
     "status": "ok",
     "timestamp": 1757761340805,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "5d737308",
    "outputId": "91617ee2-ddb4-400a-f10c-8cfd35139bd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Linear SVR: {'C': 1}\n",
      "Best cross-validation score (Negative MSE) for Linear SVR: -3026.59246296212\n",
      "\n",
      "Best parameters for Polynomial SVR: {'C': 0.1, 'coef0': 10, 'degree': 3}\n",
      "Best cross-validation score (Negative MSE) for Polynomial SVR: -2990.0804032835194\n",
      "\n",
      "Best parameters for RBF SVR: {'C': 100, 'gamma': 0.01}\n",
      "Best cross-validation score (Negative MSE) for RBF SVR: -2937.417624601308\n",
      "\n",
      "Best parameters for Sigmoid SVR: {'C': 10, 'coef0': 0, 'gamma': 0.1}\n",
      "Best cross-validation score (Negative MSE) for Sigmoid SVR: -3074.81708677377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Define parameter grids for each SVR kernel (reusing from previous subtask)\n",
    "param_grid_linear_reg = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "param_grid_poly_reg = {'C': [0.1, 1, 10], 'degree': [2, 3, 4], 'coef0': [0, 1, 10]}\n",
    "param_grid_rbf_reg = {'C': [0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1]}\n",
    "param_grid_sigmoid_reg = {'C': [0.1, 1, 10], 'gamma': [0.001, 0.01, 0.1], 'coef0': [0, 1]}\n",
    "\n",
    "# Instantiate and fit GridSearchCV for each kernel\n",
    "grid_search_linear_reg = GridSearchCV(SVR(kernel='linear'), param_grid_linear_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_linear_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_poly_reg = GridSearchCV(SVR(kernel='poly'), param_grid_poly_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_poly_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_rbf_reg = GridSearchCV(SVR(kernel='rbf'), param_grid_rbf_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_rbf_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "grid_search_sigmoid_reg = GridSearchCV(SVR(kernel='sigmoid'), param_grid_sigmoid_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_sigmoid_reg.fit(X_reg_scaled, y_reg)\n",
    "\n",
    "# Print the best hyperparameters and best cross-validation score for each kernel\n",
    "print(\"Best parameters for Linear SVR:\", grid_search_linear_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Linear SVR:\", grid_search_linear_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Polynomial SVR:\", grid_search_poly_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Polynomial SVR:\", grid_search_poly_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for RBF SVR:\", grid_search_rbf_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for RBF SVR:\", grid_search_rbf_reg.best_score_)\n",
    "\n",
    "print(\"\\nBest parameters for Sigmoid SVR:\", grid_search_sigmoid_reg.best_params_)\n",
    "print(\"Best cross-validation score (Negative MSE) for Sigmoid SVR:\", grid_search_sigmoid_reg.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "769617fe"
   },
   "source": [
    "**Reasoning**:\n",
    "Explain regression validation techniques and error metrics (MSE, R-squared), reusing content from the \"Regression validation and error metrics\" subtask.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1757761351482,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "a6b592ce",
    "outputId": "911033c8-90f5-4f8e-b6e6-0d6b6955e8cf"
   },
   "source": [
    "## SVR Model Validation and Error Metrics\n",
    "\n",
    "To evaluate the performance of our SVR models and understand how well they predict the continuous target variable on unseen data, we use **Cross-Validation** and specific regression error metrics. Cross-validation, particularly k-Fold Cross-Validation, provides a more reliable estimate of performance by training and evaluating the model on different subsets of the data.\n",
    "\n",
    "We will use the following error metrics to evaluate the tuned SVR models:\n",
    "\n",
    "*   **Mean Squared Error (MSE):** Measures the average of the squared differences between the predicted and actual values. Lower MSE indicates better performance.\n",
    "    $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "*   **R-squared ($R^2$) Score:** Represents the proportion of the variance in the target variable that is predictable from the features. A score closer to 1 indicates a better fit.\n",
    "    $R^2 = 1 - \\frac{\\text{Sum of Squares of Residuals}}{\\text{Total Sum of Squares}} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "\n",
    "These metrics, combined with cross-validation, provide a comprehensive assessment of the SVR models' predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 595,
     "status": "ok",
     "timestamp": 1757761367822,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "274235ed",
    "outputId": "84f39401-20f9-4508-892c-f574270716c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Cross-validation Results for Tuned SVR Models:\n",
      "Linear SVR: MSE = 3026.5925, R-squared = 0.4772\n",
      "Polynomial SVR: MSE = 2990.0804, R-squared = 0.4831\n",
      "RBF SVR: MSE = 2937.4176, R-squared = 0.4923\n",
      "Sigmoid SVR: MSE = 3074.8171, R-squared = 0.4697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Retrieve the best estimators from the completed GridSearchCV objects\n",
    "best_svr_linear = grid_search_linear_reg.best_estimator_\n",
    "best_svr_poly = grid_search_poly_reg.best_estimator_\n",
    "best_svr_rbf = grid_search_rbf_reg.best_estimator_\n",
    "best_svr_sigmoid = grid_search_sigmoid_reg.best_estimator_\n",
    "\n",
    "# Define the scoring metrics for regression\n",
    "scoring_reg = ['neg_mean_squared_error', 'r2']\n",
    "\n",
    "# Perform 5-fold cross-validation for each best SVR estimator\n",
    "cv_results_linear_reg = cross_validate(best_svr_linear, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_poly_reg = cross_validate(best_svr_poly, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_rbf_reg = cross_validate(best_svr_rbf, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "cv_results_sigmoid_reg = cross_validate(best_svr_sigmoid, X_reg_scaled, y_reg, cv=5, scoring=scoring_reg)\n",
    "\n",
    "# Calculate the average MSE and R-squared for each kernel\n",
    "average_mse_linear_reg = -cv_results_linear_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_linear_reg = cv_results_linear_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_poly_reg = -cv_results_poly_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_poly_reg = cv_results_poly_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_rbf_reg = -cv_results_rbf_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_rbf_reg = cv_results_rbf_reg['test_r2'].mean()\n",
    "\n",
    "average_mse_sigmoid_reg = -cv_results_sigmoid_reg['test_neg_mean_squared_error'].mean()\n",
    "average_r2_sigmoid_reg = cv_results_sigmoid_reg['test_r2'].mean()\n",
    "\n",
    "# Print the average MSE and R-squared for each SVR kernel\n",
    "print(\"Average Cross-validation Results for Tuned SVR Models:\")\n",
    "print(f\"Linear SVR: MSE = {average_mse_linear_reg:.4f}, R-squared = {average_r2_linear_reg:.4f}\")\n",
    "print(f\"Polynomial SVR: MSE = {average_mse_poly_reg:.4f}, R-squared = {average_r2_poly_reg:.4f}\")\n",
    "print(f\"RBF SVR: MSE = {average_mse_rbf_reg:.4f}, R-squared = {average_r2_rbf_reg:.4f}\")\n",
    "print(f\"Sigmoid SVR: MSE = {average_mse_sigmoid_reg:.4f}, R-squared = {average_r2_sigmoid_reg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1757761379226,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "bcf9fd43",
    "outputId": "db82af6b-9e42-4ebd-b473-04ac8741ff6c"
   },
   "source": [
    "## Feature Importance in Linear SVR\n",
    "\n",
    "For Support Vector Regression models employing a **linear kernel**, the concept of feature importance can be derived directly from the model's **coefficients**. A linear SVR model fits a linear function to predict the target, and the coefficients represent the weights assigned to each input feature.\n",
    "\n",
    "*   The **absolute value of a coefficient** indicates the magnitude of a feature's influence on the predicted target value. A larger absolute coefficient means the feature has a stronger impact on the prediction.\n",
    "*   The **sign of a coefficient** indicates the direction of the relationship: a positive sign suggests that increasing the feature's value increases the predicted target, while a negative sign suggests the opposite.\n",
    "\n",
    "By examining the absolute values of these coefficients, we can rank features by their importance in the linear SVR model. This interpretation is specific to the linear kernel; for non-linear kernels, assessing feature importance is generally more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 416
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1757761391235,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "b6da5a59",
    "outputId": "632672bf-65c0-48da-e387-fa195a9b96a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Feature Importance for Linear SVR:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s5     21.541068\n",
       "bmi    20.861853\n",
       "bp     15.922391\n",
       "sex    11.955362\n",
       "s3      8.627998\n",
       "s4      6.509319\n",
       "s2      4.805351\n",
       "s6      4.200074\n",
       "s1      3.949230\n",
       "age     0.104373\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Access the coefficients of the trained best_svr_linear model\n",
    "# The coefficients are stored in the 'coef_' attribute. Since it's a single output, we take the first row [0].\n",
    "feature_importance_linear_reg = pd.Series(best_svr_linear.coef_[0], index=X_reg.columns)\n",
    "\n",
    "# Calculate the absolute value to represent the magnitude of feature importance\n",
    "feature_importance_linear_reg = feature_importance_linear_reg.abs()\n",
    "\n",
    "# Sort the feature importance values in descending order\n",
    "sorted_feature_importance_linear_reg = feature_importance_linear_reg.sort_values(ascending=False)\n",
    "\n",
    "# Print and display the sorted feature importance\n",
    "print(\"Sorted Feature Importance for Linear SVR:\")\n",
    "display(sorted_feature_importance_linear_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0590430a"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The breast cancer dataset has 569 instances and 31 features (including the target), with no missing values. Features are numerical and were scaled for SVM.\n",
    "*   For breast cancer classification, after hyperparameter tuning with GridSearchCV and 5-fold cross-validation:\n",
    "    *   The Polynomial kernel achieved the highest average accuracy (0.9807) and F1-score (0.9849), along with a high average recall (0.9972).\n",
    "    *   The RBF kernel performed comparably well (Accuracy: 0.9789, F1-score: 0.9834).\n",
    "    *   The Linear kernel also showed strong performance (Accuracy: 0.9754, F1-score: 0.9806).\n",
    "    *   The Sigmoid kernel had slightly lower average scores across metrics (Accuracy: 0.9701, F1-score: 0.9765).\n",
    "*   For linear SVM classification on the breast cancer dataset, features like `worst concave points`, `worst perimeter`, and `mean concave points` have the highest absolute coefficients, indicating they are the most influential in the linear decision boundary.\n",
    "*   The diabetes dataset for regression contains 442 instances and 10 numerical features, along with a continuous target variable. The features were standardized.\n",
    "*   For diabetes regression, after hyperparameter tuning with GridSearchCV and 5-fold cross-validation using negative mean squared error:\n",
    "    *   The RBF kernel achieved the lowest average Mean Squared Error (2937.42) and the highest average R-squared score (0.4639), indicating the best performance among the tested kernels.\n",
    "    *   The Polynomial kernel had the next best average MSE (2990.08) and R-squared (0.4541).\n",
    "    *   The Linear kernel followed with average MSE (3026.59) and R-squared (0.4474).\n",
    "    *   The Sigmoid kernel had the highest average MSE (3074.82) and lowest average R-squared (0.4383), indicating the poorest performance.\n",
    "*   For linear SVR on the diabetes dataset, features such as `s5` (possibly log of serum triglycerides level), `bmi` (body mass index), and `bp` (average blood pressure) have the largest absolute coefficients, suggesting they are the most important features influencing the linear prediction of disease progression.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   For the breast cancer classification task, the Polynomial and RBF kernels appear to be the best choices based on the evaluation metrics. Further analysis could involve comparing these models with other classification algorithms.\n",
    "*   For the diabetes regression task, the RBF kernel demonstrated the best performance among the tested SVR kernels. It would be beneficial to investigate the impact of the `epsilon` hyperparameter during tuning, as it was not included in the provided grids, and this parameter significantly affects SVR performance.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqBwX8iZclsEnYgmXrIH3b",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
