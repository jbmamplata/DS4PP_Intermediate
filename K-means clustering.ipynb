{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76f5ebf2"
   },
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "executionInfo": {
     "elapsed": 71,
     "status": "ok",
     "timestamp": 1757767869092,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "0c9923e2",
    "outputId": "ed1d71b5-27e2-41c9-a476-8b425ee1e2ce"
   },
   "source": [
    "K-means clustering is an unsupervised machine learning algorithm used to partition a dataset into *k* distinct, non-overlapping subgroups or \"clusters\". The goal is to group data points such that those within the same cluster are similar to each other, while those in different clusters are dissimilar.\n",
    "\n",
    "## How K-Means Works\n",
    "\n",
    "The algorithm follows an iterative process:\n",
    "\n",
    "1.  **Initialization:** Choose *k* initial centroids, which are randomly selected data points or points strategically placed in the data space.\n",
    "2.  **Assignment:** Each data point is assigned to the nearest centroid based on a distance metric (commonly Euclidean distance). This forms *k* initial clusters.\n",
    "3.  **Update:** The centroids of each cluster are recalculated as the mean of all data points assigned to that cluster.\n",
    "4.  **Iteration:** Steps 2 and 3 are repeated until the centroids no longer change significantly or a maximum number of iterations is reached. This indicates that the clusters have stabilized.\n",
    "\n",
    "## Applications of K-Means Clustering\n",
    "\n",
    "K-means is a versatile algorithm with numerous applications across various domains:\n",
    "\n",
    "*   **Customer Segmentation:** Grouping customers based on purchasing behavior, demographics, or other characteristics for targeted marketing.\n",
    "*   **Image Compression:** Reducing the number of colors in an image by grouping similar colors into clusters and representing each cluster by its centroid color.\n",
    "*   **Anomaly Detection:** Identifying unusual data points that do not fit into any of the established clusters.\n",
    "*   **Document Analysis:** Clustering documents based on their content to discover topics or themes.\n",
    "*   **Genomic Analysis:** Grouping genes with similar expression patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63d2bb9c"
   },
   "source": [
    "## Load and explore datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4945,
     "status": "ok",
     "timestamp": 1757767889769,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "d1d9310f",
    "outputId": "cc7f8692-fef7-4841-bce4-e81461d19ce7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits, load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "digits_df = pd.DataFrame(digits.data, columns=[f'pixel_{i}' for i in range(digits.data.shape[1])])\n",
    "digits_df['target'] = digits.target\n",
    "\n",
    "print(\"Digits Dataset:\")\n",
    "display(digits_df.head())\n",
    "print(\"\\nDigits Dataset Info:\")\n",
    "digits_df.info()\n",
    "print(\"\\nDigits Dataset Missing Values:\")\n",
    "print(digits_df.isnull().sum().sum())\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "breast_cancer_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "breast_cancer_df['target'] = breast_cancer.target\n",
    "\n",
    "print(\"\\nBreast Cancer Dataset:\")\n",
    "display(breast_cancer_df.head())\n",
    "print(\"\\nBreast Cancer Dataset Info:\")\n",
    "breast_cancer_df.info()\n",
    "print(\"\\nBreast Cancer Dataset Missing Values:\")\n",
    "print(breast_cancer_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1757767900452,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "e2d61a61",
    "outputId": "a892173c-f43d-444f-8743-e00f26d67cf1"
   },
   "source": [
    "## Digits Dataset\n",
    "\n",
    "The Digits dataset is a collection of handwritten digits images, originally from the UCI Machine Learning Repository. It is a widely used dataset for classification and clustering tasks.\n",
    "\n",
    "*   **Source:** UCI Machine Learning Repository (often included with scikit-learn)\n",
    "*   **Number of Features:** 64\n",
    "*   **Feature Representation:** Each feature represents the intensity of a pixel in an 8x8 grayscale image of a handwritten digit. The pixel intensity values range from 0 to 16.\n",
    "\n",
    "## Breast Cancer Wisconsin (Diagnostic) Dataset\n",
    "\n",
    "The Breast Cancer Wisconsin (Diagnostic) dataset contains features computed from digitized images of fine needle aspirate (FNA) of a breast mass. These features describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "*   **Source:** UCI Machine Learning Repository (often included with scikit-learn)\n",
    "*   **Number of Features:** 30\n",
    "*   **Feature Representation:** The features represent various measurements of cell nuclei, such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. For each of these characteristics, the mean, standard error, and \"worst\" (mean of the three largest values) are computed, resulting in 30 features. The target variable indicates whether the tumor is malignant (0) or benign (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02915505"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1757767914835,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "75184f5c",
    "outputId": "574b947b-5322-4c76-8f9a-ae01a1464d6a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate features (X) and target (y) for digits_df\n",
    "X_digits = digits_df.drop('target', axis=1)\n",
    "y_digits = digits_df['target']\n",
    "\n",
    "# Separate features (X) and target (y) for breast_cancer_df\n",
    "X_breast_cancer = breast_cancer_df.drop('target', axis=1)\n",
    "y_breast_cancer = breast_cancer_df['target']\n",
    "\n",
    "# Scale the feature data for digits_df\n",
    "scaler_digits = StandardScaler()\n",
    "X_digits_scaled = scaler_digits.fit_transform(X_digits)\n",
    "\n",
    "# Scale the feature data for breast_cancer_df\n",
    "scaler_breast_cancer = StandardScaler()\n",
    "X_breast_cancer_scaled = scaler_breast_cancer.fit_transform(X_breast_cancer)\n",
    "\n",
    "print(\"Scaled Digits Features (first 5 rows):\")\n",
    "display(pd.DataFrame(X_digits_scaled, columns=X_digits.columns).head())\n",
    "\n",
    "print(\"\\nScaled Breast Cancer Features (first 5 rows):\")\n",
    "display(pd.DataFrame(X_breast_cancer_scaled, columns=X_breast_cancer.columns).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c5dded6"
   },
   "source": [
    "## Implement k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2199,
     "status": "ok",
     "timestamp": 1757767932586,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "17843268",
    "outputId": "099030c6-2182-473e-ac93-e93fc40237b8"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Instantiate and fit KMeans for scaled digits data\n",
    "kmeans_digits = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
    "kmeans_digits.fit(X_digits_scaled)\n",
    "\n",
    "# Instantiate and fit KMeans for scaled breast cancer data\n",
    "kmeans_breast_cancer = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "kmeans_breast_cancer.fit(X_breast_cancer_scaled)\n",
    "\n",
    "# Store cluster labels\n",
    "digits_cluster_labels = kmeans_digits.labels_\n",
    "breast_cancer_cluster_labels = kmeans_breast_cancer.labels_\n",
    "\n",
    "print(\"Digits Cluster Labels (first 10):\")\n",
    "print(digits_cluster_labels[:10])\n",
    "\n",
    "print(\"\\nBreast Cancer Cluster Labels (first 10):\")\n",
    "print(breast_cancer_cluster_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cabd0fe"
   },
   "source": [
    "## Determine optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4405,
     "status": "ok",
     "timestamp": 1757767953222,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "6f5b5952"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate inertia for scaled digits data (Elbow Method)\n",
    "inertia = []\n",
    "k_range_digits = range(1, 21)\n",
    "for k in k_range_digits:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_digits_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Calculate silhouette scores for scaled breast cancer data\n",
    "silhouette_scores = []\n",
    "k_range_breast_cancer = range(2, 11)\n",
    "for k in k_range_breast_cancer:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_breast_cancer_scaled)\n",
    "    score = silhouette_score(X_breast_cancer_scaled, kmeans.labels_)\n",
    "    silhouette_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1757767962023,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "342c7e69",
    "outputId": "206b8149-db35-442f-b11a-45dae3b983fb"
   },
   "outputs": [],
   "source": [
    "# Generate elbow plot for digits data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range_digits, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k (Digits Dataset)')\n",
    "plt.xticks(k_range_digits)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Generate silhouette plot for breast cancer data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range_breast_cancer, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal k (Breast Cancer Dataset)')\n",
    "plt.xticks(k_range_breast_cancer)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46e8f2b9"
   },
   "source": [
    "## Evaluate clustering performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1757767975315,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "63c4f483"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1757767983444,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "727cbc94",
    "outputId": "67f04492-944d-47d7-feea-034de2fe93f8"
   },
   "outputs": [],
   "source": [
    "# Calculate and print metrics for the digits dataset\n",
    "homogeneity = homogeneity_score(y_digits, digits_cluster_labels)\n",
    "completeness = completeness_score(y_digits, digits_cluster_labels)\n",
    "v_measure = v_measure_score(y_digits, digits_cluster_labels)\n",
    "\n",
    "print(f\"Digits Dataset Clustering Metrics:\")\n",
    "print(f\"Homogeneity: {homogeneity:.4f}\")\n",
    "print(f\"Completeness: {completeness:.4f}\")\n",
    "print(f\"V-measure: {v_measure:.4f}\")\n",
    "\n",
    "# Calculate and print metrics for the breast cancer dataset\n",
    "adjusted_rand = adjusted_rand_score(y_breast_cancer, breast_cancer_cluster_labels)\n",
    "adjusted_mutual_info = adjusted_mutual_info_score(y_breast_cancer, breast_cancer_cluster_labels)\n",
    "\n",
    "print(f\"\\nBreast Cancer Dataset Clustering Metrics:\")\n",
    "print(f\"Adjusted Rand Index: {adjusted_rand:.4f}\")\n",
    "print(f\"Adjusted Mutual Information: {adjusted_mutual_info:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72955813"
   },
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1757768003459,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "b04fdea6",
    "outputId": "756ec156-1065-47d2-f396-1b715098810b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate centroids for digits\n",
    "digits_centroids = kmeans_digits.cluster_centers_\n",
    "\n",
    "# Calculate mean feature values for each pixel within each cluster for digits\n",
    "digits_cluster_means = pd.DataFrame(X_digits_scaled).groupby(digits_cluster_labels).mean()\n",
    "\n",
    "# Reshape mean pixel values for visualization (optional, for later use)\n",
    "# This is a list of arrays, where each array is an 8x8 image\n",
    "digits_mean_images = [mean_values.reshape(8, 8) for mean_values in digits_cluster_means.values]\n",
    "\n",
    "# Calculate mean and standard deviation for each feature for each cluster for breast cancer\n",
    "breast_cancer_cluster_summary = pd.DataFrame(X_breast_cancer_scaled, columns=X_breast_cancer.columns).groupby(breast_cancer_cluster_labels).agg(['mean', 'std'])\n",
    "\n",
    "print(\"Digits Cluster Centroids (first 5 rows, first 5 columns):\")\n",
    "display(pd.DataFrame(digits_centroids).head())\n",
    "\n",
    "print(\"\\nDigits Cluster Mean Feature Values (first 5 clusters, first 5 pixels):\")\n",
    "display(digits_cluster_means.head())\n",
    "\n",
    "print(\"\\nBreast Cancer Cluster Summary (Mean and Std Dev per feature per cluster):\")\n",
    "display(breast_cancer_cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1757768015354,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "248c2cb7",
    "outputId": "56497994-13ba-4ed3-f662-5155085da2e2"
   },
   "outputs": [],
   "source": [
    "# Visualize mean digit images for each cluster\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits_mean_images[i], cmap='gray')\n",
    "    ax.set_title(f'Cluster {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Mean Image for Each Cluster (Digits Dataset)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the distribution of feature values within each cluster for breast cancer\n",
    "# We already have the mean and std dev summary in breast_cancer_cluster_summary\n",
    "# Let's calculate the difference in means between the two clusters for each feature\n",
    "mean_diff = breast_cancer_cluster_summary.loc[0, (slice(None), 'mean')] - breast_cancer_cluster_summary.loc[1, (slice(None), 'mean')]\n",
    "\n",
    "# Sort features by the absolute difference in means\n",
    "sorted_mean_diff = mean_diff.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nFeatures sorted by absolute difference in mean across clusters (Breast Cancer):\")\n",
    "print(sorted_mean_diff)\n",
    "\n",
    "# Interpretation: Features with larger absolute differences in means across clusters are more important in distinguishing between the two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbc67cb9"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 996
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1757768040796,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "78c9abb6",
    "outputId": "4d32727b-e938-40e6-f280-3e9cae98b0c2"
   },
   "source": [
    "## Hyperparameter Tuning for K-Means Clustering\n",
    "\n",
    "Hyperparameter tuning is a crucial step in optimizing the performance of machine learning algorithms, and k-means clustering is no exception. Hyperparameters are parameters that are set *before* the learning process begins, as opposed to model parameters which are learned from the data. Tuning these hyperparameters helps to find the best configuration for a given dataset and clustering objective.\n",
    "\n",
    "The main hyperparameters of the KMeans algorithm in scikit-learn include:\n",
    "\n",
    "*   `n_clusters`: The most critical hyperparameter, representing the number of clusters to form.\n",
    "*   `init`: The method for initializing the centroids.\n",
    "*   `n_init`: The number of times the k-means algorithm will be run with different centroid seeds.\n",
    "\n",
    "### Tuning `n_clusters`\n",
    "\n",
    "Determining the optimal number of clusters (`n_clusters`) is a key aspect of tuning k-means. As demonstrated in a previous step, methods like the **Elbow Method** and the **Silhouette Score** are commonly used for this purpose.\n",
    "\n",
    "*   The **Elbow Method** plots the inertia (within-cluster sum of squares) against the number of clusters. The \"elbow point\" in the plot, where the rate of decrease in inertia slows down significantly, is often considered an indicator of the optimal number of clusters.\n",
    "*   The **Silhouette Score** measures how similar a data point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined and well-separated clusters. By calculating the silhouette score for different values of `n_clusters`, we can choose the number that yields the highest score.\n",
    "\n",
    "Both of these methods are essentially forms of hyperparameter tuning for `n_clusters`, allowing us to explore different values and evaluate their impact on the clustering structure.\n",
    "\n",
    "### Understanding `init`\n",
    "\n",
    "The `init` parameter controls the method used to place the initial centroids. The choice of initialization method can significantly impact the final clustering result, as k-means can converge to different local optima depending on the starting positions of the centroids.\n",
    "\n",
    "*   `'k-means++'`: This is the default and generally recommended initialization method. It smartly selects initial centroids that are far away from each other, which tends to lead to better and more consistent results than random initialization.\n",
    "*   `'random'`: This method chooses `n_clusters` observations randomly from the data for the initial centroids. This method is more susceptible to converging to suboptimal solutions.\n",
    "\n",
    "### The Importance of `n_init`\n",
    "\n",
    "The `n_init` parameter specifies the number of times the k-means algorithm will be run with different initial centroids. For each run, a different random seed is used for centroid initialization (unless `init` is a callable). The final result is the best output of `n_init` consecutive runs in terms of inertia.\n",
    "\n",
    "Running the algorithm multiple times with different initializations is important because:\n",
    "\n",
    "*   K-means is not guaranteed to find the global minimum of the inertia. It can get stuck in local minima.\n",
    "*   By running multiple times with different starting points, we increase the chances of finding a better, potentially the global, optimum.\n",
    "\n",
    "In our previous implementation of k-means, we set `n_init=10`. This means the algorithm was run 10 different times with different random initializations, and the best result (lowest inertia) among these 10 runs was chosen as the final model. This helps to mitigate the sensitivity of k-means to the initial centroid placement.\n",
    "\n",
    "### Other Considerations\n",
    "\n",
    "Other hyperparameters and considerations for tuning KMeans include:\n",
    "\n",
    "*   `max_iter`: The maximum number of iterations for the algorithm to run for a single initialization. If the centroids converge before this, the algorithm stops.\n",
    "*   `tol`: The tolerance for declaring convergence. The algorithm stops when the change in inertia is below this threshold.\n",
    "\n",
    "Tuning these parameters can further refine the clustering process, although `n_clusters`, `init`, and `n_init` are typically the most impactful for the quality and robustness of the final clusters.\n",
    "\n",
    "### Conclusion on Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is essential for obtaining meaningful and robust clustering results with KMeans. While methods like the Elbow method and Silhouette score help determine the optimal `n_clusters`, understanding and appropriately setting parameters like `init` and `n_init` are equally important. By running the algorithm multiple times with a smart initialization strategy, we increase the reliability of the clustering outcome and reduce the risk of converging to a poor local optimum. Effective tuning ensures that the k-means model captures the underlying structure of the data as accurately as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55b90d8a"
   },
   "source": [
    "## Validation and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1757768068594,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "50a81e23",
    "outputId": "851ece9c-eec8-4cf8-fa24-149138d09d1a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# 1. Validate Digits Clustering\n",
    "print(\"Validating Digits Clustering:\")\n",
    "# Create a confusion matrix (cross-tabulation)\n",
    "confusion_matrix_digits = pd.crosstab(y_digits, digits_cluster_labels)\n",
    "print(\"Confusion Matrix (True Digits vs. Cluster Labels):\")\n",
    "display(confusion_matrix_digits)\n",
    "\n",
    "# Map cluster labels to true labels for accuracy calculation\n",
    "# We need to find the best mapping between cluster labels and true digit labels.\n",
    "# This can be done by finding the assignment that maximizes the sum of the confusion matrix diagonal.\n",
    "# We can use the Hungarian algorithm (linear_sum_assignment) for this.\n",
    "\n",
    "# Create a cost matrix (negative of the confusion matrix) for the Hungarian algorithm\n",
    "cost_matrix = -confusion_matrix_digits.values\n",
    "\n",
    "# Find the optimal assignment of cluster labels to true labels\n",
    "row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# The matched labels are row_ind (true labels) and col_ind (cluster labels)\n",
    "# Create a mapping from cluster label to true label\n",
    "cluster_to_digit_mapping = {cluster_label: true_label for cluster_label, true_label in zip(col_ind, row_ind)}\n",
    "print(\"\\nOptimal Cluster to Digit Mapping:\")\n",
    "print(cluster_to_digit_mapping)\n",
    "\n",
    "# Map the predicted cluster labels to the assigned true labels\n",
    "mapped_digits_labels = np.array([cluster_to_digit_mapping[label] for label in digits_cluster_labels])\n",
    "\n",
    "# Calculate \"accuracy\" based on the best mapping\n",
    "accuracy_digits = accuracy_score(y_digits, mapped_digits_labels)\n",
    "print(f\"\\n'Accuracy' of Digits Clustering (with optimal mapping): {accuracy_digits:.4f}\")\n",
    "\n",
    "\n",
    "# 2. Validate Breast Cancer Clustering\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Validating Breast Cancer Clustering:\")\n",
    "# Create a confusion matrix (cross-tabulation)\n",
    "confusion_matrix_breast_cancer = pd.crosstab(y_breast_cancer, breast_cancer_cluster_labels)\n",
    "print(\"Confusion Matrix (True Diagnosis vs. Cluster Labels):\")\n",
    "display(confusion_matrix_breast_cancer)\n",
    "\n",
    "# Map cluster labels to true labels for accuracy calculation\n",
    "# Assuming 2 clusters correspond to 2 classes (0 and 1).\n",
    "# We can determine the mapping by seeing which cluster label has more samples from class 0, and which has more from class 1.\n",
    "# Or use the Hungarian algorithm again for robustness.\n",
    "\n",
    "# Create a cost matrix for the Hungarian algorithm (2x2)\n",
    "cost_matrix_bc = -confusion_matrix_breast_cancer.values\n",
    "\n",
    "# Find the optimal assignment\n",
    "row_ind_bc, col_ind_bc = linear_sum_assignment(cost_matrix_bc)\n",
    "\n",
    "# Create mapping from cluster label to true label (0 or 1)\n",
    "cluster_to_diagnosis_mapping = {cluster_label: true_label for cluster_label, true_label in zip(col_ind_bc, row_ind_bc)}\n",
    "print(\"\\nOptimal Cluster to Diagnosis Mapping:\")\n",
    "print(cluster_to_diagnosis_mapping)\n",
    "\n",
    "# Map the predicted cluster labels to the assigned true labels\n",
    "mapped_breast_cancer_labels = np.array([cluster_to_diagnosis_mapping[label] for label in breast_cancer_cluster_labels])\n",
    "\n",
    "# Calculate \"accuracy\" based on the best mapping\n",
    "accuracy_breast_cancer = accuracy_score(y_breast_cancer, mapped_breast_cancer_labels)\n",
    "print(f\"\\n'Accuracy' of Breast Cancer Clustering (with optimal mapping): {accuracy_breast_cancer:.4f}\")\n",
    "\n",
    "\n",
    "# 3. Interpret the Clusters (using previously calculated values)\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Interpreting Clusters:\")\n",
    "\n",
    "print(\"\\nDigits Dataset Cluster Interpretation:\")\n",
    "# Relate mean images to true digits (already done visually in feature importance step)\n",
    "# Here, we can reiterate based on the mapping\n",
    "print(\"Based on the optimal mapping:\")\n",
    "for cluster, digit in cluster_to_digit_mapping.items():\n",
    "    print(f\"  Cluster {cluster} likely corresponds to digit {digit}\")\n",
    "\n",
    "# For breast cancer, describe typical feature values for each cluster\n",
    "print(\"\\nBreast Cancer Dataset Cluster Interpretation:\")\n",
    "print(\"Summary of Mean Feature Values per Cluster:\")\n",
    "display(breast_cancer_cluster_summary.loc[:, (slice(None), 'mean')])\n",
    "\n",
    "# Relate to benign (target=1) and malignant (target=0) characteristics\n",
    "# We know from the confusion matrix and mapping which cluster corresponds to benign and which to malignant.\n",
    "# Let's assume cluster 0 maps to malignant (0) and cluster 1 maps to benign (1) based on the hungarian algorithm result.\n",
    "# (Verify this assumption with the cluster_to_diagnosis_mapping)\n",
    "\n",
    "malignant_cluster = cluster_to_diagnosis_mapping[0] if 0 in cluster_to_diagnosis_mapping else None\n",
    "benign_cluster = cluster_to_diagnosis_mapping[1] if 1 in cluster_to_diagnosis_mapping else None\n",
    "\n",
    "\n",
    "if malignant_cluster is not None and benign_cluster is not None:\n",
    "    print(f\"\\nCluster {list(cluster_to_diagnosis_mapping.keys())[list(cluster_to_diagnosis_mapping.values()).index(0)]} likely corresponds to Malignant (0) and Cluster {list(cluster_to_diagnosis_mapping.keys())[list(cluster_to_diagnosis_mapping.values()).index(1)]} likely corresponds to Benign (1).\")\n",
    "    print(\"\\nTypical characteristics of the Malignant cluster:\")\n",
    "    display(breast_cancer_cluster_summary.loc[list(cluster_to_diagnosis_mapping.keys())[list(cluster_to_diagnosis_mapping.values()).index(0)], (slice(None), 'mean')].sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nTypical characteristics of the Benign cluster:\")\n",
    "    display(breast_cancer_cluster_summary.loc[list(cluster_to_diagnosis_mapping.keys())[list(cluster_to_diagnosis_mapping.values()).index(1)], (slice(None), 'mean')].sort_values(ascending=False))\n",
    "\n",
    "    print(\"\\nComparison based on feature importance (features with largest mean difference):\")\n",
    "    display(sorted_mean_diff.head())\n",
    "    print(\"Features with larger mean values in the Malignant cluster (relative to Benign) are characteristic of malignancy.\")\n",
    "    print(\"Features with smaller mean values in the Malignant cluster (relative to Benign) are characteristic of malignancy.\")\n",
    "\n",
    "\n",
    "# 4. Summarize findings in markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "executionInfo": {
     "elapsed": 1965,
     "status": "ok",
     "timestamp": 1757768088250,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "5fee2c2c",
    "outputId": "3310e56e-5501-4a76-8ac4-c81d6527595d"
   },
   "outputs": [],
   "source": [
    "%%markdown\n",
    "## Validation and Interpretation of K-Means Clustering Results\n",
    "\n",
    "This section summarizes the validation and interpretation of the K-Means clustering results for the digits and breast cancer datasets, comparing the unsupervised cluster assignments to the known true labels.\n",
    "\n",
    "### Digits Dataset\n",
    "\n",
    "For the digits dataset, with 10 clusters, the clustering results were validated by comparing the predicted cluster labels to the true digit labels (0-9) using a confusion matrix (cross-tabulation).\n",
    "\n",
    "**Validation:**\n",
    "\n",
    "*   The confusion matrix shows the distribution of true digit labels within each cluster. Ideally, each cluster would primarily contain samples of a single digit.\n",
    "*   Due to the unsupervised nature of k-means, the cluster labels themselves are arbitrary (e.g., cluster 0 might correspond to digit 8). To assess performance against true labels, we found the optimal mapping between cluster labels and true digit labels using the Hungarian algorithm, which maximizes the number of correctly assigned samples.\n",
    "*   With this optimal mapping, the \"accuracy\" of the clustering was calculated to be **0.6628**. This indicates that approximately 66.3% of the samples were assigned to the cluster that optimally corresponds to their true digit label. The remaining samples represent misclassifications by the clustering algorithm.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "*   Based on the optimal mapping, we identified which cluster is most likely to represent each digit (e.g., Cluster 2 corresponds to digit 0, Cluster 3 corresponds to digit 1, etc.).\n",
    "*   Interpreting the clusters involves examining the mean image of the samples within each cluster (as visualized in the feature importance step). The mean images visually confirm that the clusters capture distinct digit patterns, although some clusters may show characteristics of multiple digits, contributing to the misclassification rate. For instance, clusters corresponding to digits that are visually similar (like '3' and '8' or '5' and '6') might have more mixed membership.\n",
    "\n",
    "### Breast Cancer Dataset\n",
    "\n",
    "For the breast cancer dataset, with 2 clusters, the clustering results were validated by comparing the predicted cluster labels to the true diagnosis labels (0 for malignant, 1 for benign) using a confusion matrix.\n",
    "\n",
    "**Validation:**\n",
    "\n",
    "*   The confusion matrix shows how the two clusters align with the benign and malignant diagnoses.\n",
    "*   Using the Hungarian algorithm to find the optimal mapping between cluster labels and true diagnosis labels (where one cluster maps to malignant and the other to benign), the \"accuracy\" of the clustering was calculated to be **0.9051**. This high accuracy suggests that k-means was quite effective at separating the samples into two groups that largely correspond to the benign and malignant classes.\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "*   Based on the optimal mapping, we identified which cluster corresponds to the malignant diagnosis and which corresponds to the benign diagnosis. From the confusion matrix, one cluster predominantly contains malignant cases, and the other predominantly contains benign cases.\n",
    "*   Interpreting the clusters involves examining the typical feature values (mean values for each feature within each cluster), as summarized previously.\n",
    "*   The cluster corresponding to **Malignant (0)** is characterized by **higher mean values** for many features, particularly those related to the size, shape, and irregularity of the cell nuclei, such as `mean concave points`, `worst concave points`, `mean concavity`, `worst concavity`, `worst perimeter`, and `mean compactness`. These features were identified as having the largest differences in means between the two clusters in the feature importance analysis.\n",
    "*   Conversely, the cluster corresponding to **Benign (1)** is characterized by **lower mean values** for these same features, reflecting smaller, smoother, and more regular cell nuclei.\n",
    "\n",
    "**Overall Summary:**\n",
    "\n",
    "K-Means clustering was applied to both the digits and breast cancer datasets with many features. Validation using confusion matrices and optimal mapping revealed that the clustering results align reasonably well with the true labels, achieving an \"accuracy\" of approximately 66.3% for digits (with 10 clusters) and a high \"accuracy\" of 90.5% for breast cancer (with 2 clusters). The interpretation of the clusters, guided by the mean feature values and previous feature importance analysis, showed that the clusters capture meaningful patterns in the data. For digits, clusters correspond to distinct handwritten numbers, while for breast cancer, the two clusters effectively differentiate between benign and malignant cases based on key characteristics of cell nuclei."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a977b1d"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1757768118293,
     "user": {
      "displayName": "Jonathan Mamplata",
      "userId": "03615677832131703370"
     },
     "user_tz": -480
    },
    "id": "ab908020",
    "outputId": "1a489d95-5b85-4a53-bc4a-93dd244344de"
   },
   "source": [
    "# Summary and Conclusion\n",
    "\n",
    "This notebook explored the application of K-Means clustering, an unsupervised machine learning algorithm, to two datasets with a significant number of features: the Digits dataset and the Breast Cancer Wisconsin (Diagnostic) dataset. K-Means aims to partition data into *k* distinct clusters based on feature similarity, making it suitable for discovering inherent groupings in datasets where the true labels are unknown or for exploratory data analysis.\n",
    "\n",
    "## Summary of the K-Means Clustering Process and Findings\n",
    "\n",
    "The analysis involved several key steps:\n",
    "\n",
    "1.  **Data Loading and Exploration:** We loaded the Digits dataset (64 features representing pixel intensities of handwritten digits) and the Breast Cancer dataset (30 features representing characteristics of cell nuclei). Initial exploration confirmed that both datasets contained numerical features and no missing values, making them suitable for K-Means.\n",
    "2.  **Preprocessing (Scaling):** To ensure that all features contributed equally to the distance calculations in K-Means, the features of both datasets were scaled using `StandardScaler`. This step is crucial for algorithms sensitive to the scale of features, like K-Means.\n",
    "3.  **Implementing K-Means:** K-Means clustering was applied to the scaled data. For the Digits dataset, we initially chose 10 clusters, corresponding to the 10 possible digits. For the Breast Cancer dataset, we chose 2 clusters, corresponding to the two possible diagnoses (benign or malignant). The algorithm was run multiple times with different centroid initializations (`n_init=10`) to find a more robust solution.\n",
    "4.  **Determining the Number of Clusters:** While the number of clusters was initially set based on domain knowledge (10 for digits, 2 for breast cancer), methods like the Elbow Method (for digits) and the Silhouette Score (for breast cancer) were demonstrated as approaches to programmatically determine a suitable number of clusters when the true number is unknown. These methods help identify values of *k* where the clustering structure is most coherent.\n",
    "5.  **Evaluating Performance:** Since the true labels were available for both datasets, we evaluated the clustering performance by comparing the predicted cluster labels to the true labels. Metrics such as Homogeneity, Completeness, and V-measure were used for the Digits dataset, and Adjusted Rand Index and Adjusted Mutual Information were used for the Breast Cancer dataset. These metrics quantify the agreement between the clustering results and the ground truth, accounting for the arbitrary nature of cluster labels.\n",
    "6.  **Feature Importance Analysis:** To understand which features were most influential in forming the clusters, we examined the cluster centroids and the mean feature values within each cluster. For the Digits dataset, visualizing the mean images for each cluster revealed how pixel intensities differentiated the digits. For the Breast Cancer dataset, comparing the mean feature values between the two clusters highlighted features with the largest differences, indicating their importance in distinguishing between the groups.\n",
    "7.  **Hyperparameter Tuning:** The importance of hyperparameter tuning, particularly for `n_clusters`, `init`, and `n_init`, was discussed. The use of the Elbow Method and Silhouette Score for `n_clusters` was reiterated, and the benefits of using `'k-means++'` initialization and running the algorithm multiple times (`n_init`) were explained as strategies to improve the quality and robustness of the clustering.\n",
    "8.  **Validation and Interpretation:** Finally, we validated the clustering by creating confusion matrices and finding the optimal mapping between cluster labels and true labels using the Hungarian algorithm. This allowed us to calculate an \"accuracy\" score, providing a more intuitive measure of how well the unsupervised clusters aligned with the known classes. We then interpreted the meaning of the clusters based on this mapping and the characteristic feature values within each cluster.\n",
    "\n",
    "## Validation Results\n",
    "\n",
    "The validation step provided valuable insights into the effectiveness of K-Means in grouping the data according to the true underlying classes:\n",
    "\n",
    "*   **Digits Dataset:** With an optimal mapping between the 10 cluster labels and the 10 true digit labels, the clustering achieved an \"accuracy\" of **0.6628**. This indicates that while K-Means successfully identified distinct groupings corresponding to the digits, there was still a notable amount of overlap or misclassification, likely due to the visual similarity between certain digits and the inherent difficulty of distinguishing them based solely on pixel intensities in some cases. The evaluation metrics like Homogeneity (0.6498), Completeness (0.6941), and V-measure (0.6712) also reflected a moderate level of agreement with the true labels.\n",
    "*   **Breast Cancer Dataset:** For the Breast Cancer dataset, with 2 clusters optimally mapped to the benign and malignant diagnoses, the clustering achieved a high \"accuracy\" of **0.9051**. This demonstrates that K-Means was highly effective at separating the samples into two groups that strongly corresponded to the benign and malignant tumor types. The Adjusted Rand Index (0.6536) and Adjusted Mutual Information (0.5318), while lower than accuracy (as they are stricter measures of cluster similarity independent of labeling), still indicated a substantial agreement between the clustering and the true diagnoses.\n",
    "\n",
    "These results imply that K-Means can be a powerful tool for discovering meaningful groupings in data, especially when the clusters are relatively well-separated in the feature space, as seen with the breast cancer data. For more complex or overlapping structures, like the digits, the performance may be more modest.\n",
    "\n",
    "## Cluster Interpretation\n",
    "\n",
    "Interpreting the meaning of the clusters provided context to the numerical results:\n",
    "\n",
    "*   **Digits Dataset:** The 10 clusters generally corresponded to the 10 different handwritten digits (0-9). The mean images for each cluster visually represented the typical appearance of the digits assigned to that cluster. Features (pixels) that varied significantly in intensity across these mean images were crucial for distinguishing the digits.\n",
    "*   **Breast Cancer Dataset:** The two clusters clearly separated the benign and malignant cases. The cluster corresponding to the malignant diagnosis was characterized by significantly higher values for features related to the size, irregularity, and complexity of the cell nuclei (e.g., radius, perimeter, area, concavity, concave points). The cluster corresponding to the benign diagnosis showed lower values for these features. This aligns with medical understanding that malignant tumors tend to exhibit more abnormal cell characteristics.\n",
    "\n",
    "## Concluding Discussion on K-Means for High-Dimensional Data\n",
    "\n",
    "Applying K-Means to datasets with many features, as demonstrated with the Digits (64 features) and Breast Cancer (30 features) datasets, presents both strengths and limitations:\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "*   **Simplicity and Efficiency:** K-Means is computationally efficient and relatively easy to understand and implement, making it a good starting point for clustering high-dimensional data.\n",
    "*   **Scalability:** It scales well to large datasets, which is important when dealing with many features and samples.\n",
    "*   **Interpretability (to some extent):** By examining cluster centroids and feature distributions within clusters, we can gain insights into the characteristics that define each group, as shown in our analysis.\n",
    "*   **Effective for Well-Separated Clusters:** As demonstrated by the high \"accuracy\" on the breast cancer dataset, K-Means can be very effective at identifying clusters that are relatively distinct in the feature space.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "*   **Assumes Spherical Clusters:** K-Means assumes that clusters are roughly spherical and equally sized, which may not hold true for all datasets. In high dimensions, the concept of distance can become less intuitive, potentially affecting the performance of distance-based algorithms like K-Means.\n",
    "*   **Sensitivity to Initialization:** The final clustering result can depend on the initial placement of centroids, although using `k-means++` and multiple initializations (`n_init`) helps mitigate this.\n",
    "*   **Requires Pre-specifying *k*:** A significant challenge is determining the optimal number of clusters *k* beforehand, especially without prior knowledge. While methods like the Elbow method and Silhouette score provide guidance, the \"true\" number of clusters in unsupervised settings can be ambiguous.\n",
    "*   **Sensitivity to Outliers:** Outliers can disproportionately influence the position of centroids.\n",
    "*   **Difficulty with Complex Structures:** K-Means struggles with clusters of irregular shapes or those that are intertwined.\n",
    "\n",
    "**Real-World Applications:**\n",
    "\n",
    "Despite its limitations, K-Means remains a widely used algorithm in real-world applications involving high-dimensional data, such as image segmentation (pixels as features), document clustering (word frequencies as features), customer segmentation (demographic and behavioral data as features), and anomaly detection. Its effectiveness often depends on the nature of the data and the degree to which the underlying clusters conform to the algorithm's assumptions.\n",
    "\n",
    "In conclusion, K-Means clustering provides a valuable approach for exploring the structure of high-dimensional datasets. While careful preprocessing, determination of *k*, and consideration of its assumptions are necessary, it can effectively reveal meaningful groupings and provide interpretable insights, as demonstrated by its successful application to the Digits and Breast Cancer datasets. For more complex data structures, other clustering algorithms might be more appropriate, but K-Means serves as a fundamental and often effective tool in the data scientist's toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58e251a4"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   **Data Loading and Preprocessing:** Two datasets with multiple features were successfully loaded: the Digits dataset (1797 samples, 64 features) and the Breast Cancer dataset (569 samples, 30 features). Both datasets had no missing values. Feature scaling was applied using `StandardScaler` to prepare the data for K-Means clustering.\n",
    "*   **K-Means Implementation:** K-Means clustering was applied to the scaled data. Initially, 10 clusters were used for the Digits dataset (corresponding to the 10 digits) and 2 clusters for the Breast Cancer dataset (corresponding to benign and malignant diagnoses).\n",
    "*   **Determining Optimal Clusters:** The Elbow Method was used for the Digits dataset (plotting inertia vs. number of clusters), and the Silhouette Score was used for the Breast Cancer dataset (plotting silhouette score vs. number of clusters) to help determine a suitable number of clusters.\n",
    "*   **Clustering Evaluation:** Using the true labels for both datasets, the clustering performance was evaluated. For the Digits dataset (10 clusters), metrics included Homogeneity (0.6498), Completeness (0.6941), and V-measure (0.6712). For the Breast Cancer dataset (2 clusters), metrics included Adjusted Rand Index (0.6536) and Adjusted Mutual Information (0.5318).\n",
    "*   **Validation with Optimal Mapping:** By finding the optimal mapping between cluster labels and true labels using the Hungarian algorithm, an \"accuracy\" was calculated: 0.6628 for the Digits dataset and 0.9051 for the Breast Cancer dataset. This shows how well the unsupervised clusters align with the known classes.\n",
    "*   **Feature Importance:** Analysis of cluster centroids and mean feature values per cluster helped understand feature importance. For Digits, visualizing mean images per cluster showed how pixel intensities differentiate digits. For Breast Cancer, features with the largest differences in mean values between the two clusters (e.g., `mean concave points`, `worst concave points`, `mean concavity`, `worst concavity`, `worst perimeter`) were identified as most important in distinguishing benign from malignant cases.\n",
    "*   **Hyperparameter Tuning:** Key K-Means hyperparameters (`n_clusters`, `init`, `n_init`) were discussed. The importance of `n_init` (running the algorithm multiple times) was highlighted to avoid local minima, and `'k-means++'` was noted as the preferred initialization method.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   While K-Means showed strong performance in separating benign and malignant cases in the Breast Cancer dataset, its performance was more modest on the Digits dataset. This suggests that K-Means is more effective when clusters are relatively well-separated and approximately spherical, which might be less true for handwritten digits compared to the distinct biological features in the breast cancer data.\n",
    "*   For the Digits dataset, exploring other clustering algorithms that can handle more complex shapes or incorporating dimensionality reduction techniques (like PCA) before K-Means could potentially improve the clustering accuracy and better separate visually similar digits.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMkAqCZ4bm9j88dloQzsyz/",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
